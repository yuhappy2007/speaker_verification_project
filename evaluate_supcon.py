# """
# SupConæ¨¡å‹æµ‹è¯•è„šæœ¬
# - æ”¯æŒ1000 pairså¿«é€Ÿæµ‹è¯•
# - ä¸åŸè®ºæ–‡baselineå¯¹æ¯”
# - ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
#
# ç”¨æ³•:
#     python evaluate_supcon.py --checkpoint checkpoints_supcon_200batch/final_model_supcon.pth
# """
#
# import torch
# import torch.nn.functional as F
# import numpy as np
# from pathlib import Path
# import argparse
# from tqdm import tqdm
# import json
#
# # æ·»åŠ scriptsè·¯å¾„
# import sys
#
# sys.path.append('scripts')
#
# from models_supcon import RobustEmbeddingMLP
# from speaker_embedding import SpeakerEmbeddingExtractor
# from speech_enhancer import SpeechEnhancer
# from dataset import VoxCelebMusanDataset
#
# # åŸè®ºæ–‡baselineç»“æœ(ä»è®ºæ–‡Table 1å¤åˆ¶)
# PAPER_BASELINE = {
#     'Noise_-15dB': {
#         'Noisy': 34.71,
#         'Enhanced': 32.77,
#         'Triplet(Paper)': 25.21
#     },
#     'Music_-15dB': {
#         'Noisy': 44.46,
#         'Enhanced': 41.86,
#         'Triplet(Paper)': 36.37
#     },
#     'Babble_-15dB': {
#         'Noisy': 46.72,
#         'Enhanced': 47.48,
#         'Triplet(Paper)': 37.21
#     }
# }
#
#
# def load_supcon_model(checkpoint_path, device):
#     """åŠ è½½SupConæ¨¡å‹"""
#     print(f'Loading SupCon model from: {checkpoint_path}')
#
#     # åˆ›å»ºMLP
#     mlp = RobustEmbeddingMLP(embedding_dim=192).to(device)
#
#     # åŠ è½½æƒé‡
#     checkpoint = torch.load(checkpoint_path, map_location=device)
#     mlp.load_state_dict(checkpoint['model_state_dict'])
#     mlp.eval()
#
#     print(f"âœ… Model loaded")
#     print(f"   - Training loss: {checkpoint['loss']:.4f}")
#     print(f"   - Batch: {checkpoint['batch']}")
#     print(f"   - Temperature: {checkpoint['config']['temperature']}")
#
#     return mlp, checkpoint
#
#
# def extract_supcon_embedding(audio, speaker_model, enhancer, mlp, device):
#     """
#     ä½¿ç”¨SupConæ¨¡å‹æå–robust embedding
#
#     æµç¨‹:
#     1. æå–noisy embedding
#     2. è¯­éŸ³å¢å¼º
#     3. æå–enhanced embedding
#     4. é€šè¿‡MLPèåˆ
#     """
#     with torch.no_grad():
#         # æå–noisy embedding
#         noisy_emb = speaker_model.extract_embedding(audio_tensor=audio)
#         noisy_emb = F.normalize(noisy_emb.squeeze(), p=2, dim=0)
#
#         # è¯­éŸ³å¢å¼º
#         enhanced_audio = enhancer.enhance_audio(audio, sr=16000)
#
#         # æå–enhanced embedding
#         enhanced_emb = speaker_model.extract_embedding(audio_tensor=enhanced_audio)
#         enhanced_emb = F.normalize(enhanced_emb.squeeze(), p=2, dim=0)
#
#         # é€šè¿‡MLPèåˆ(ä¼šè‡ªåŠ¨L2å½’ä¸€åŒ–)
#         noisy_emb = noisy_emb.unsqueeze(0).to(device)
#         enhanced_emb = enhanced_emb.unsqueeze(0).to(device)
#         robust_emb = mlp(noisy_emb, enhanced_emb)
#
#         return robust_emb.squeeze().cpu()
#
#
# def compute_eer(scores, labels):
#     """
#     è®¡ç®—EER(Equal Error Rate)
#
#     å‚æ•°:
#         scores: ç›¸ä¼¼åº¦åˆ†æ•°åˆ—è¡¨
#         labels: æ ‡ç­¾åˆ—è¡¨(1=same speaker, 0=different speaker)
#     """
#     # è½¬æ¢ä¸ºnumpy
#     scores = np.array(scores)
#     labels = np.array(labels)
#
#     # æŒ‰åˆ†æ•°æ’åº
#     sorted_indices = np.argsort(scores)
#     scores = scores[sorted_indices]
#     labels = labels[sorted_indices]
#
#     # è®¡ç®—FARå’ŒFRR
#     n_positive = np.sum(labels == 1)
#     n_negative = np.sum(labels == 0)
#
#     best_eer = 1.0
#     best_threshold = 0.0
#
#     for i, threshold in enumerate(scores):
#         # False Accept Rate: è´Ÿæ ·æœ¬ä¸­å¾—åˆ†>=thresholdçš„æ¯”ä¾‹
#         far = np.sum((scores >= threshold) & (labels == 0)) / n_negative
#
#         # False Reject Rate: æ­£æ ·æœ¬ä¸­å¾—åˆ†<thresholdçš„æ¯”ä¾‹
#         frr = np.sum((scores < threshold) & (labels == 1)) / n_positive
#
#         # EERæ˜¯FARå’ŒFRRç›¸ç­‰çš„ç‚¹
#         eer = (far + frr) / 2
#
#         if abs(far - frr) < abs(best_eer - 0.5):
#             best_eer = eer
#             best_threshold = threshold
#
#     return best_eer * 100, best_threshold  # è¿”å›ç™¾åˆ†æ¯”
#
#
# def evaluate_supcon(mlp, speaker_model, enhancer, test_dataset, num_pairs=1000, device='cuda'):
#     """
#     è¯„ä¼°SupConæ¨¡å‹
#
#     å‚æ•°:
#         num_pairs: æµ‹è¯•pairæ•°é‡(é»˜è®¤1000ç”¨äºå¿«é€Ÿæµ‹è¯•)
#
#     è¿”å›:
#         eer: Equal Error Rate (%)
#     """
#     print(f'\nğŸ“Š Evaluating SupCon model on {num_pairs} pairs...')
#
#     mlp.eval()
#     scores = []
#     labels = []
#
#     # ç”Ÿæˆæµ‹è¯•pairs
#     test_pairs = []
#     for i in range(num_pairs // 2):
#         # æ­£æ ·æœ¬å¯¹(åŒä¸€è¯´è¯äºº)
#         idx1 = np.random.randint(len(test_dataset))
#         sample1 = test_dataset[idx1]
#
#         # æ‰¾åŒä¸€è¯´è¯äººçš„å¦ä¸€ä¸ªæ ·æœ¬
#         same_speaker_samples = [j for j, s in enumerate(test_dataset.samples)
#                                 if s['speaker_id'] == sample1['speaker_id'] and j != idx1]
#         if same_speaker_samples:
#             idx2 = np.random.choice(same_speaker_samples)
#             test_pairs.append((idx1, idx2, 1))
#
#         # è´Ÿæ ·æœ¬å¯¹(ä¸åŒè¯´è¯äºº)
#         idx3 = np.random.randint(len(test_dataset))
#         sample3 = test_dataset[idx3]
#
#         # æ‰¾ä¸åŒè¯´è¯äºº
#         diff_speaker_samples = [j for j, s in enumerate(test_dataset.samples)
#                                 if s['speaker_id'] != sample3['speaker_id']]
#         if diff_speaker_samples:
#             idx4 = np.random.choice(diff_speaker_samples)
#             test_pairs.append((idx3, idx4, 0))
#
#     # è¯„ä¼°æ¯ä¸ªpair
#     for idx1, idx2, label in tqdm(test_pairs, desc='Testing'):
#         sample1 = test_dataset[idx1]
#         sample2 = test_dataset[idx2]
#
#         # æå–robust embeddings
#         emb1 = extract_supcon_embedding(
#             sample1['anchor_noisy'], speaker_model, enhancer, mlp, device
#         )
#         emb2 = extract_supcon_embedding(
#             sample2['anchor_noisy'], speaker_model, enhancer, mlp, device
#         )
#
#         # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
#         score = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()
#
#         scores.append(score)
#         labels.append(label)
#
#     # è®¡ç®—EER
#     eer, threshold = compute_eer(scores, labels)
#
#     print(f'âœ… Evaluation complete')
#     print(f'   - EER: {eer:.2f}%')
#     print(f'   - Threshold: {threshold:.4f}')
#     print(f'   - Avg score (same): {np.mean([s for s, l in zip(scores, labels) if l == 1]):.4f}')
#     print(f'   - Avg score (diff): {np.mean([s for s, l in zip(scores, labels) if l == 0]):.4f}')
#
#     return eer, threshold
#
#
# def print_comparison_table(supcon_eer, noise_type='-15dB'):
#     """
#     æ‰“å°ä¸åŸè®ºæ–‡baselineçš„å¯¹æ¯”è¡¨æ ¼
#
#     å‚æ•°:
#         supcon_eer: SupConæ¨¡å‹çš„EER
#         noise_type: å™ªå£°ç±»å‹(ç”¨äºä»baselineä¸­æŸ¥æ‰¾)
#     """
#     print('\n' + '=' * 80)
#     print('ğŸ“Š COMPARISON WITH PAPER BASELINE')
#     print('=' * 80)
#
#     # è·å–å¯¹åº”å™ªå£°ç±»å‹çš„baseline
#     baseline_key = f'Noise_{noise_type}'
#     if baseline_key in PAPER_BASELINE:
#         baseline = PAPER_BASELINE[baseline_key]
#
#         print(f'\n{"Method":<25} {"EER (%)":<15} {"vs Triplet":<15} {"vs Noisy":<15}')
#         print('-' * 80)
#
#         # Noisy baseline
#         print(f'{"Noisy (Paper)":<25} {baseline["Noisy"]:<15.2f} {"":<15} {"":<15}')
#
#         # Enhanced baseline
#         print(f'{"Enhanced (Paper)":<25} {baseline["Enhanced"]:<15.2f} {"":<15} {"":<15}')
#
#         # Triplet Loss baseline
#         triplet_eer = baseline['Triplet(Paper)']
#         print(f'{"Triplet Loss (Paper)":<25} {triplet_eer:<15.2f} {"":<15} {"":<15}')
#
#         # SupCon (ä½ çš„ç»“æœ)
#         vs_triplet = supcon_eer - triplet_eer
#         vs_noisy = supcon_eer - baseline['Noisy']
#
#         status = 'âœ…' if supcon_eer < triplet_eer else 'âš ï¸'
#         print(f'{f"SupCon (Yours) {status}":<25} {supcon_eer:<15.2f} {vs_triplet:+.2f} {"":<6} {vs_noisy:+.2f}')
#
#         print('-' * 80)
#
#         # åˆ†æ
#         print('\nğŸ” ANALYSIS:')
#         if supcon_eer < triplet_eer:
#             improvement = (triplet_eer - supcon_eer) / triplet_eer * 100
#             print(f'âœ… SupCon is BETTER than Triplet Loss by {improvement:.1f}%')
#             print(f'   - Absolute improvement: {triplet_eer - supcon_eer:.2f}% EER')
#         elif supcon_eer > triplet_eer:
#             degradation = (supcon_eer - triplet_eer) / triplet_eer * 100
#             print(f'âš ï¸  SupCon is worse than Triplet Loss by {degradation:.1f}%')
#             print(f'   - Consider: adjusting temperature, more training epochs')
#         else:
#             print(f'â– SupCon performs similarly to Triplet Loss')
#
#         if supcon_eer < baseline['Noisy']:
#             improvement_noisy = (baseline['Noisy'] - supcon_eer) / baseline['Noisy'] * 100
#             print(f'âœ… SupCon improves Noisy baseline by {improvement_noisy:.1f}%')
#
#     print('=' * 80)
#
#
# def main():
#     parser = argparse.ArgumentParser(description='Evaluate SupCon model')
#     parser.add_argument('--checkpoint', type=str,
#                         default='checkpoints_supcon_200batch/final_model_supcon.pth',
#                         help='Path to SupCon checkpoint')
#     parser.add_argument('--num_pairs', type=int, default=1000,
#                         help='Number of test pairs (default: 1000 for fast testing)')
#     parser.add_argument('--noise_type', type=str, default='-15dB',
#                         help='Noise type for comparison (default: -15dB)')
#     parser.add_argument('--voxceleb_dir', type=str, default='data/voxceleb1',
#                         help='VoxCeleb dataset directory')
#     parser.add_argument('--musan_dir', type=str, default='data/musan',
#                         help='MUSAN dataset directory')
#     args = parser.parse_args()
#
#     device = 'cuda' if torch.cuda.is_available() else 'cpu'
#
#     print('=' * 80)
#     print('ğŸ§ª SUPCON MODEL EVALUATION')
#     print('=' * 80)
#     print(f'Checkpoint: {args.checkpoint}')
#     print(f'Test pairs: {args.num_pairs}')
#     print(f'Device: {device}')
#     print('=' * 80)
#
#     # åŠ è½½æ¨¡å‹
#     print('\n[1/4] Loading models...')
#     mlp, checkpoint = load_supcon_model(args.checkpoint, device)
#     speaker_model = SpeakerEmbeddingExtractor('ecapa')
#     enhancer = SpeechEnhancer()
#
#     # åŠ è½½æµ‹è¯•æ•°æ®
#     print('\n[2/4] Loading test dataset...')
#     test_dataset = VoxCelebMusanDataset(
#         args.voxceleb_dir,
#         args.musan_dir,
#         split='test',
#         snr_range=(-15, -15)  # å›ºå®š-15dBç”¨äºä¸è®ºæ–‡å¯¹æ¯”
#     )
#     print(f'âœ… Test samples: {len(test_dataset)}')
#
#     # è¯„ä¼°
#     print('\n[3/4] Evaluating...')
#     supcon_eer, threshold = evaluate_supcon(
#         mlp, speaker_model, enhancer, test_dataset,
#         num_pairs=args.num_pairs, device=device
#     )
#
#     # å¯¹æ¯”
#     print('\n[4/4] Comparing with baseline...')
#     print_comparison_table(supcon_eer, noise_type=args.noise_type)
#
#     # ä¿å­˜ç»“æœ
#     results_file = Path(args.checkpoint).parent / 'evaluation_results.json'
#     results = {
#         'supcon_eer': supcon_eer,
#         'threshold': threshold,
#         'num_pairs': args.num_pairs,
#         'paper_baseline': PAPER_BASELINE,
#         'checkpoint': str(args.checkpoint)
#     }
#
#     with open(results_file, 'w') as f:
#         json.dump(results, f, indent=2)
#
#     print(f'\nğŸ’¾ Results saved: {results_file}')
#     print('\nâœ… Evaluation complete!')
#
#
# if __name__ == '__main__':
#     main()
"""
SupConæ¨¡å‹æµ‹è¯•è„šæœ¬ (ä¿®å¤ç‰ˆ)
- æ”¯æŒ1000 pairså¿«é€Ÿæµ‹è¯•
- ä¸åŸè®ºæ–‡baselineå¯¹æ¯”
- ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼

ç”¨æ³•:
    python evaluate_supcon.py --checkpoint checkpoints_supcon_200batch/final_model_supcon.pth
"""

import torch
import torch.nn.functional as F
import numpy as np
from pathlib import Path
import argparse
from tqdm import tqdm
import json

# æ·»åŠ scriptsè·¯å¾„
import sys

sys.path.append('scripts')

from models_supcon import RobustEmbeddingMLP
from speaker_embedding import SpeakerEmbeddingExtractor
from speech_enhancer import SpeechEnhancer
from dataset import VoxCelebMusanDataset

# åŸè®ºæ–‡baselineç»“æœ(ä»è®ºæ–‡Table 1å¤åˆ¶)
PAPER_BASELINE = {
    'Noise_-15dB': {
        'Noisy': 34.71,
        'Enhanced': 32.77,
        'Triplet(Paper)': 25.21
    },
    'Music_-15dB': {
        'Noisy': 44.46,
        'Enhanced': 41.86,
        'Triplet(Paper)': 36.37
    },
    'Babble_-15dB': {
        'Noisy': 46.72,
        'Enhanced': 47.48,
        'Triplet(Paper)': 37.21
    }
}


def load_supcon_model(checkpoint_path, device):
    """åŠ è½½SupConæ¨¡å‹"""
    print(f'Loading SupCon model from: {checkpoint_path}')

    # åˆ›å»ºMLP
    mlp = RobustEmbeddingMLP(embedding_dim=192).to(device)

    # åŠ è½½æƒé‡
    checkpoint = torch.load(checkpoint_path, map_location=device)
    mlp.load_state_dict(checkpoint['model_state_dict'])
    mlp.eval()

    print(f"âœ… Model loaded")
    print(f"   - Training loss: {checkpoint['loss']:.4f}")
    print(f"   - Batch: {checkpoint['batch']}")
    print(f"   - Temperature: {checkpoint['config']['temperature']}")

    return mlp, checkpoint


def extract_supcon_embedding(audio, speaker_model, enhancer, mlp, device):
    """
    ä½¿ç”¨SupConæ¨¡å‹æå–robust embedding

    æµç¨‹:
    1. æå–noisy embedding
    2. è¯­éŸ³å¢å¼º
    3. æå–enhanced embedding
    4. é€šè¿‡MLPèåˆ
    """
    with torch.no_grad():
        # æå–noisy embedding
        noisy_emb = speaker_model.extract_embedding(audio_tensor=audio)
        noisy_emb = F.normalize(noisy_emb.squeeze(), p=2, dim=0)

        # è¯­éŸ³å¢å¼º
        enhanced_audio = enhancer.enhance_audio(audio, sr=16000)

        # æå–enhanced embedding
        enhanced_emb = speaker_model.extract_embedding(audio_tensor=enhanced_audio)
        enhanced_emb = F.normalize(enhanced_emb.squeeze(), p=2, dim=0)

        # é€šè¿‡MLPèåˆ(ä¼šè‡ªåŠ¨L2å½’ä¸€åŒ–)
        noisy_emb = noisy_emb.unsqueeze(0).to(device)
        enhanced_emb = enhanced_emb.unsqueeze(0).to(device)
        robust_emb = mlp(noisy_emb, enhanced_emb)

        return robust_emb.squeeze().cpu()


def compute_eer(scores, labels):
    """
    è®¡ç®—EER(Equal Error Rate)

    å‚æ•°:
        scores: ç›¸ä¼¼åº¦åˆ†æ•°åˆ—è¡¨
        labels: æ ‡ç­¾åˆ—è¡¨(1=same speaker, 0=different speaker)
    """
    # è½¬æ¢ä¸ºnumpy
    scores = np.array(scores)
    labels = np.array(labels)

    # æŒ‰åˆ†æ•°æ’åº
    sorted_indices = np.argsort(scores)
    scores = scores[sorted_indices]
    labels = labels[sorted_indices]

    # è®¡ç®—FARå’ŒFRR
    n_positive = np.sum(labels == 1)
    n_negative = np.sum(labels == 0)

    best_eer = 1.0
    best_threshold = 0.0

    for i, threshold in enumerate(scores):
        # False Accept Rate: è´Ÿæ ·æœ¬ä¸­å¾—åˆ†>=thresholdçš„æ¯”ä¾‹
        far = np.sum((scores >= threshold) & (labels == 0)) / n_negative

        # False Reject Rate: æ­£æ ·æœ¬ä¸­å¾—åˆ†<thresholdçš„æ¯”ä¾‹
        frr = np.sum((scores < threshold) & (labels == 1)) / n_positive

        # EERæ˜¯FARå’ŒFRRç›¸ç­‰çš„ç‚¹
        eer = (far + frr) / 2

        if abs(far - frr) < abs(best_eer - 0.5):
            best_eer = eer
            best_threshold = threshold

    return best_eer * 100, best_threshold  # è¿”å›ç™¾åˆ†æ¯”


def evaluate_supcon(mlp, speaker_model, enhancer, test_dataset, num_pairs=1000, device='cuda'):
    """
    è¯„ä¼°SupConæ¨¡å‹

    å‚æ•°:
        num_pairs: æµ‹è¯•pairæ•°é‡(é»˜è®¤1000ç”¨äºå¿«é€Ÿæµ‹è¯•)

    è¿”å›:
        eer: Equal Error Rate (%)
    """
    print(f'\nğŸ“Š Evaluating SupCon model on {num_pairs} pairs...')

    mlp.eval()
    scores = []
    labels = []

    # âœ… ä¿®å¤: æ„å»ºspeaker_idåˆ°æ ·æœ¬ç´¢å¼•çš„æ˜ å°„
    print('Building speaker index...')
    speaker_to_samples = {}
    for idx in range(len(test_dataset)):
        # è·å–speaker_id(é€šè¿‡ä¸´æ—¶åŠ è½½)
        sample = test_dataset[idx]
        speaker_id = sample['speaker_id']

        if speaker_id not in speaker_to_samples:
            speaker_to_samples[speaker_id] = []
        speaker_to_samples[speaker_id].append(idx)

    print(f'Found {len(speaker_to_samples)} unique speakers in test set')

    # ç”Ÿæˆæµ‹è¯•pairs
    print('Generating test pairs...')
    test_pairs = []
    speakers = list(speaker_to_samples.keys())

    for i in range(num_pairs // 2):
        # æ­£æ ·æœ¬å¯¹(åŒä¸€è¯´è¯äºº)
        speaker1 = np.random.choice(speakers)
        samples1 = speaker_to_samples[speaker1]

        if len(samples1) >= 2:
            idx1, idx2 = np.random.choice(samples1, size=2, replace=False)
            test_pairs.append((idx1, idx2, 1))

        # è´Ÿæ ·æœ¬å¯¹(ä¸åŒè¯´è¯äºº)
        speaker2, speaker3 = np.random.choice(speakers, size=2, replace=False)
        idx3 = np.random.choice(speaker_to_samples[speaker2])
        idx4 = np.random.choice(speaker_to_samples[speaker3])
        test_pairs.append((idx3, idx4, 0))

    print(f'Generated {len(test_pairs)} test pairs')

    # è¯„ä¼°æ¯ä¸ªpair
    for idx1, idx2, label in tqdm(test_pairs, desc='Testing'):
        sample1 = test_dataset[idx1]
        sample2 = test_dataset[idx2]

        # æå–robust embeddings
        emb1 = extract_supcon_embedding(
            sample1['anchor_noisy'], speaker_model, enhancer, mlp, device
        )
        emb2 = extract_supcon_embedding(
            sample2['anchor_noisy'], speaker_model, enhancer, mlp, device
        )

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        score = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()

        scores.append(score)
        labels.append(label)

    # è®¡ç®—EER
    eer, threshold = compute_eer(scores, labels)

    print(f'âœ… Evaluation complete')
    print(f'   - EER: {eer:.2f}%')
    print(f'   - Threshold: {threshold:.4f}')
    print(f'   - Avg score (same): {np.mean([s for s, l in zip(scores, labels) if l == 1]):.4f}')
    print(f'   - Avg score (diff): {np.mean([s for s, l in zip(scores, labels) if l == 0]):.4f}')

    return eer, threshold


def print_comparison_table(supcon_eer, noise_type='-15dB'):
    """
    æ‰“å°ä¸åŸè®ºæ–‡baselineçš„å¯¹æ¯”è¡¨æ ¼

    å‚æ•°:
        supcon_eer: SupConæ¨¡å‹çš„EER
        noise_type: å™ªå£°ç±»å‹(ç”¨äºä»baselineä¸­æŸ¥æ‰¾)
    """
    print('\n' + '=' * 80)
    print('ğŸ“Š COMPARISON WITH PAPER BASELINE')
    print('=' * 80)

    # è·å–å¯¹åº”å™ªå£°ç±»å‹çš„baseline
    baseline_key = f'Noise_{noise_type}'
    if baseline_key in PAPER_BASELINE:
        baseline = PAPER_BASELINE[baseline_key]

        print(f'\n{"Method":<25} {"EER (%)":<15} {"vs Triplet":<15} {"vs Noisy":<15}')
        print('-' * 80)

        # Noisy baseline
        print(f'{"Noisy (Paper)":<25} {baseline["Noisy"]:<15.2f} {"":<15} {"":<15}')

        # Enhanced baseline
        print(f'{"Enhanced (Paper)":<25} {baseline["Enhanced"]:<15.2f} {"":<15} {"":<15}')

        # Triplet Loss baseline
        triplet_eer = baseline['Triplet(Paper)']
        print(f'{"Triplet Loss (Paper)":<25} {triplet_eer:<15.2f} {"":<15} {"":<15}')

        # SupCon (ä½ çš„ç»“æœ)
        vs_triplet = supcon_eer - triplet_eer
        vs_noisy = supcon_eer - baseline['Noisy']

        status = 'âœ…' if supcon_eer < triplet_eer else 'âš ï¸'
        print(f'{f"SupCon (Yours) {status}":<25} {supcon_eer:<15.2f} {vs_triplet:+.2f} {"":<6} {vs_noisy:+.2f}')

        print('-' * 80)

        # åˆ†æ
        print('\nğŸ” ANALYSIS:')
        if supcon_eer < triplet_eer:
            improvement = (triplet_eer - supcon_eer) / triplet_eer * 100
            print(f'âœ… SupCon is BETTER than Triplet Loss by {improvement:.1f}%')
            print(f'   - Absolute improvement: {triplet_eer - supcon_eer:.2f}% EER')
        elif supcon_eer > triplet_eer:
            degradation = (supcon_eer - triplet_eer) / triplet_eer * 100
            print(f'âš ï¸  SupCon is worse than Triplet Loss by {degradation:.1f}%')
            print(f'   - Consider: adjusting temperature, more training epochs')
        else:
            print(f'â– SupCon performs similarly to Triplet Loss')

        if supcon_eer < baseline['Noisy']:
            improvement_noisy = (baseline['Noisy'] - supcon_eer) / baseline['Noisy'] * 100
            print(f'âœ… SupCon improves Noisy baseline by {improvement_noisy:.1f}%')

    print('=' * 80)


def main():
    parser = argparse.ArgumentParser(description='Evaluate SupCon model')
    parser.add_argument('--checkpoint', type=str,
                        default='checkpoints_supcon_200batch/final_model_supcon.pth',
                        help='Path to SupCon checkpoint')
    parser.add_argument('--num_pairs', type=int, default=1000,
                        help='Number of test pairs (default: 1000 for fast testing)')
    parser.add_argument('--noise_type', type=str, default='-15dB',
                        help='Noise type for comparison (default: -15dB)')
    parser.add_argument('--voxceleb_dir', type=str, default='data/voxceleb1',
                        help='VoxCeleb dataset directory')
    parser.add_argument('--musan_dir', type=str, default='data/musan',
                        help='MUSAN dataset directory')
    args = parser.parse_args()

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    print('=' * 80)
    print('ğŸ§ª SUPCON MODEL EVALUATION')
    print('=' * 80)
    print(f'Checkpoint: {args.checkpoint}')
    print(f'Test pairs: {args.num_pairs}')
    print(f'Device: {device}')
    print('=' * 80)

    # åŠ è½½æ¨¡å‹
    print('\n[1/4] Loading models...')
    mlp, checkpoint = load_supcon_model(args.checkpoint, device)
    speaker_model = SpeakerEmbeddingExtractor('ecapa')
    enhancer = SpeechEnhancer()

    # åŠ è½½æµ‹è¯•æ•°æ®
    print('\n[2/4] Loading test dataset...')
    test_dataset = VoxCelebMusanDataset(
        args.voxceleb_dir,
        args.musan_dir,
        split='test',
        snr_range=(-15, -15)  # å›ºå®š-15dBç”¨äºä¸è®ºæ–‡å¯¹æ¯”
    )
    print(f'âœ… Test samples: {len(test_dataset)}')

    # è¯„ä¼°
    print('\n[3/4] Evaluating...')
    supcon_eer, threshold = evaluate_supcon(
        mlp, speaker_model, enhancer, test_dataset,
        num_pairs=args.num_pairs, device=device
    )

    # å¯¹æ¯”
    print('\n[4/4] Comparing with baseline...')
    print_comparison_table(supcon_eer, noise_type=args.noise_type)

    # ä¿å­˜ç»“æœ
    results_file = Path(args.checkpoint).parent / 'evaluation_results.json'
    results = {
        'supcon_eer': supcon_eer,
        'threshold': threshold,
        'num_pairs': args.num_pairs,
        'paper_baseline': PAPER_BASELINE,
        'checkpoint': str(args.checkpoint)
    }

    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)

    print(f'\nğŸ’¾ Results saved: {results_file}')
    print('\nâœ… Evaluation complete!')


if __name__ == '__main__':
    main()