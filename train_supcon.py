# """
# é˜¶æ®µ1ï¼šSupConæŸå¤±å®Œæ•´å®ç°
# âœ… å‚è€ƒå®˜æ–¹æºç : https://github.com/HobbitLong/SupContrast
# âœ… æ­£ç¡®å¤„ç†å½’ä¸€åŒ–
# âœ… å®Œæ•´çš„è®­ç»ƒæµç¨‹
#
# ç›®æ ‡ï¼šéªŒè¯è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ï¼Œä¸Triplet Losså¯¹æ¯”
# """
#
# import torch
# import torch.nn.functional as F
# import os
# import subprocess
# import sys
# import time
# import random
# import numpy as np
# from pathlib import Path
# from torch.utils.data import DataLoader
#
# # ç¦ç”¨DeepFilteræ—¥å¿—
# os.environ['DF_DISABLE_LOGGING'] = '1'
# original_check_output = subprocess.check_output
# subprocess.check_output = lambda *args, **kwargs: b'unknown' if args and 'git' in str(
#     args[0]) else original_check_output(*args, **kwargs)
#
# sys.path.append('scripts')
# from models_supcon import SupConTrainer  # ä½¿ç”¨SupConæ¨¡å‹
# from speaker_embedding import SpeakerEmbeddingExtractor
# from speech_enhancer import SpeechEnhancer
# from dataset import VoxCelebMusanDataset
#
#
# class Config:
#     """SupConè®­ç»ƒé…ç½®"""
#     # æ•°æ®è·¯å¾„
#     voxceleb_dir = 'data/voxceleb1'
#     musan_dir = 'data/musan'
#
#     # ===== è®­ç»ƒå‚æ•°ï¼ˆä¸åŸè®ºæ–‡ä¿æŒä¸€è‡´ä»¥ä¾¿å¯¹æ¯”ï¼‰=====
#     batch_size = 32
#     learning_rate = 1e-3
#     snr_range = (-20, 0)
#
#     # ===== SupConç‰¹æœ‰å‚æ•° =====
#     temperature = 0.07  # ğŸŒ¡ï¸ æ¸©åº¦å‚æ•°ï¼ˆå®˜æ–¹æ¨è0.07ï¼‰
#
#     # è®­ç»ƒæ§åˆ¶
#     max_batches = 200
#     checkpoint_interval = 25
#
#     # è¾“å‡ºç›®å½•
#     checkpoint_dir = 'checkpoints_supcon_200batch'
#
#     # è®¾å¤‡
#     device = 'cuda' if torch.cuda.is_available() else 'cpu'
#
#     # éšæœºç§å­
#     seed = 42
#
#
# def set_seed(seed):
#     """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯å¤ç°"""
#     random.seed(seed)
#     np.random.seed(seed)
#     torch.manual_seed(seed)
#     if torch.cuda.is_available():
#         torch.cuda.manual_seed_all(seed)
#         torch.backends.cudnn.deterministic = True
#         torch.backends.cudnn.benchmark = False
#
#
# def pad_collate(batch):
#     """
#     å¤„ç†å˜é•¿éŸ³é¢‘çš„collateå‡½æ•°
#
#     âœ… ä¿®å¤ï¼šå¤„ç†å­—ç¬¦ä¸²ç±»å‹çš„speaker_id
#     """
#     max_anchor = max([item['anchor_noisy'].shape[1] for item in batch])
#     max_positive = max([item['positive_noisy'].shape[1] for item in batch])
#     max_negative = max([item['negative_noisy'].shape[1] for item in batch])
#
#     padded_batch = {
#         'anchor_noisy': [],
#         'positive_noisy': [],
#         'negative_noisy': [],
#         'snr': [],
#         'speaker_id': []
#     }
#
#     for item in batch:
#         # Anchor padding
#         audio = item['anchor_noisy']
#         if audio.shape[1] < max_anchor:
#             padding = torch.zeros(1, max_anchor - audio.shape[1])
#             audio = torch.cat([audio, padding], dim=1)
#         padded_batch['anchor_noisy'].append(audio)
#
#         # Positive padding
#         audio = item['positive_noisy']
#         if audio.shape[1] < max_positive:
#             padding = torch.zeros(1, max_positive - audio.shape[1])
#             audio = torch.cat([audio, padding], dim=1)
#         padded_batch['positive_noisy'].append(audio)
#
#         # Negative padding
#         audio = item['negative_noisy']
#         if audio.shape[1] < max_negative:
#             padding = torch.zeros(1, max_negative - audio.shape[1])
#             audio = torch.cat([audio, padding], dim=1)
#         padded_batch['negative_noisy'].append(audio)
#
#         padded_batch['snr'].append(item['snr'])
#         padded_batch['speaker_id'].append(item['speaker_id'])
#
#     # Stackæ‰€æœ‰æ ·æœ¬
#     for key in ['anchor_noisy', 'positive_noisy', 'negative_noisy']:
#         padded_batch[key] = torch.stack(padded_batch[key])
#
#     # âœ… å…³é”®ä¿®å¤ï¼šspeaker_idæ˜¯å­—ç¬¦ä¸²ï¼Œéœ€è¦ä¿æŒåŸæ ·æˆ–æ˜ å°„ä¸ºæ•´æ•°
#     # è¿™é‡Œä¿æŒä¸ºåˆ—è¡¨ï¼Œåœ¨extractå‡½æ•°ä¸­å¤„ç†
#     # padded_batch['speaker_id'] ä¿æŒä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
#
#     return padded_batch
#
#
# def extract_and_normalize_embeddings(audio_batch, speaker_model, enhancer, device):
#     """
#     æå–embeddingså¹¶ç¡®ä¿L2å½’ä¸€åŒ–
#
#     ğŸ”‘ å…³é”®æ”¹è¿›ï¼š
#     1. æå–embeddingåç«‹å³å½’ä¸€åŒ–ï¼ˆåœ¨speaker_embedding.pyä¸­å·²ç»åšäº†ï¼‰
#     2. å†æ¬¡ç¡®è®¤å½’ä¸€åŒ–ï¼ˆé˜²æ­¢æ•°å€¼é—®é¢˜ï¼‰
#     3. ç»„ç»‡æˆSupConéœ€è¦çš„æ ¼å¼
#
#     è¿”å›æ ¼å¼ï¼š
#     - noisy_embeddings: [batch_size*3, embedding_dim]
#     - enhanced_embeddings: [batch_size*3, embedding_dim]
#     - labels: [batch_size*3]
#     """
#     batch_size = len(audio_batch['anchor_noisy'])
#
#     all_noisy_embs = []
#     all_enhanced_embs = []
#     all_labels = []
#
#     for i in range(batch_size):
#         speaker_id = audio_batch['speaker_id'][i].item()
#
#         # === å¤„ç†Anchorï¼ˆè¯´è¯äººAï¼‰===
#         anchor_noisy = audio_batch['anchor_noisy'][i]
#         anchor_enhanced = enhancer.enhance_audio(anchor_noisy, sr=16000)
#
#         # æå–embeddingï¼ˆspeaker_modelå†…éƒ¨å·²å½’ä¸€åŒ–ï¼Œä½†æˆ‘ä»¬å†ç¡®è®¤ä¸€æ¬¡ï¼‰
#         emb_noisy = speaker_model.extract_embedding(audio_tensor=anchor_noisy)
#         emb_enhanced = speaker_model.extract_embedding(audio_tensor=anchor_enhanced)
#
#         # âœ… å…³é”®ï¼šç¡®ä¿L2å½’ä¸€åŒ–ï¼ˆè®ºæ–‡è¦æ±‚ï¼ï¼‰
#         emb_noisy = emb_noisy.squeeze()
#         emb_enhanced = emb_enhanced.squeeze()
#
#         # å½’ä¸€åŒ–ï¼ˆå³ä½¿speaker_modelå·²ç»åšäº†ï¼Œå†åšä¸€æ¬¡ä»¥é˜²ä¸‡ä¸€ï¼‰
#         emb_noisy = F.normalize(emb_noisy.unsqueeze(0), p=2, dim=1).squeeze(0)
#         emb_enhanced = F.normalize(emb_enhanced.unsqueeze(0), p=2, dim=1).squeeze(0)
#
#         all_noisy_embs.append(emb_noisy)
#         all_enhanced_embs.append(emb_enhanced)
#         all_labels.append(speaker_id)
#
#         # === å¤„ç†Positiveï¼ˆè¯´è¯äººAï¼Œä¸åŒutteranceï¼‰===
#         pos_noisy = audio_batch['positive_noisy'][i]
#         pos_enhanced = enhancer.enhance_audio(pos_noisy, sr=16000)
#
#         emb_noisy = speaker_model.extract_embedding(audio_tensor=pos_noisy)
#         emb_enhanced = speaker_model.extract_embedding(audio_tensor=pos_enhanced)
#
#         emb_noisy = F.normalize(emb_noisy.squeeze().unsqueeze(0), p=2, dim=1).squeeze(0)
#         emb_enhanced = F.normalize(emb_enhanced.squeeze().unsqueeze(0), p=2, dim=1).squeeze(0)
#
#         all_noisy_embs.append(emb_noisy)
#         all_enhanced_embs.append(emb_enhanced)
#         all_labels.append(speaker_id)  # ä¸anchorç›¸åŒ
#
#         # === å¤„ç†Negativeï¼ˆè¯´è¯äººBï¼Œä¸åŒäººï¼‰===
#         neg_noisy = audio_batch['negative_noisy'][i]
#         neg_enhanced = enhancer.enhance_audio(neg_noisy, sr=16000)
#
#         emb_noisy = speaker_model.extract_embedding(audio_tensor=neg_noisy)
#         emb_enhanced = speaker_model.extract_embedding(audio_tensor=neg_enhanced)
#
#         emb_noisy = F.normalize(emb_noisy.squeeze().unsqueeze(0), p=2, dim=1).squeeze(0)
#         emb_enhanced = F.normalize(emb_enhanced.squeeze().unsqueeze(0), p=2, dim=1).squeeze(0)
#
#         all_noisy_embs.append(emb_noisy)
#         all_enhanced_embs.append(emb_enhanced)
#         # âš ï¸ æ³¨æ„ï¼šè¿™é‡Œéœ€è¦negativeçš„çœŸå®speaker_id
#         # ç”±äºdatasetè¿”å›çš„negativeæ²¡æœ‰speaker_idï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªä¸åŒçš„æ ‡è®°
#         # å®é™…ä¸­åº”è¯¥ä»datasetè·å–negativeçš„speaker_id
#         all_labels.append(speaker_id + 100000)  # ä¸´æ—¶æ–¹æ¡ˆï¼šç¡®ä¿ä¸anchor/positiveä¸åŒ
#
#     # å †å æˆbatch
#     noisy_embeddings = torch.stack(all_noisy_embs)
#     enhanced_embeddings = torch.stack(all_enhanced_embs)
#     labels = torch.tensor(all_labels)
#
#     return noisy_embeddings, enhanced_embeddings, labels
#
#
# def save_checkpoint(trainer, loss, batch_idx, total_time, config, is_final=False):
#     """ä¿å­˜checkpoint"""
#     checkpoint_dir = Path(config.checkpoint_dir)
#     checkpoint_dir.mkdir(exist_ok=True)
#
#     if is_final:
#         filename = 'final_model_supcon.pth'
#     else:
#         filename = f'checkpoint_supcon_batch_{batch_idx}.pth'
#
#     checkpoint_path = checkpoint_dir / filename
#
#     torch.save({
#         'model_state_dict': trainer.mlp.state_dict(),
#         'optimizer_state_dict': trainer.optimizer.state_dict(),
#         'loss': loss,
#         'batch': batch_idx,
#         'training_minutes': total_time,
#         'config': {
#             'loss_type': 'SupCon',
#             'batch_size': config.batch_size,
#             'learning_rate': config.learning_rate,
#             'temperature': config.temperature,
#             'optimizer': 'AdamW',
#             'snr_range': config.snr_range,
#             'max_batches': config.max_batches,
#             'seed': config.seed
#         }
#     }, checkpoint_path)
#
#     return checkpoint_path
#
#
# def main():
#     config = Config()
#     set_seed(config.seed)
#
#     print('=' * 80)
#     print('ğŸš€ SUPCON TRAINING - Supervised Contrastive Learning')
#     print('=' * 80)
#     print(f'ğŸ“Œ Loss Function: SupCon (replacing Triplet Loss)')
#     print(f'ğŸ¯ Goal: Verify training stability and convergence')
#     print(f'ğŸ“š Reference: Supervised Contrastive Learning (NeurIPS 2020)')
#     print(f'ğŸ’» Official Code: https://github.com/HobbitLong/SupContrast')
#     print('=' * 80)
#     print(f'Device: {config.device}')
#     print(f'Random seed: {config.seed}')
#     print(f'Batch size: {config.batch_size}')
#     print(f'Learning rate: {config.learning_rate}')
#     print(f'Temperature: {config.temperature} ğŸŒ¡ï¸')
#     print(f'Max batches: {config.max_batches}')
#     print('=' * 80)
#
#     # [1/3] åŠ è½½æ¨¡å‹
#     print('\n[1/3] Loading models...')
#     speaker_model = SpeakerEmbeddingExtractor('ecapa')
#     enhancer = SpeechEnhancer()
#
#     # åˆ›å»ºSupConè®­ç»ƒå™¨
#     trainer = SupConTrainer(
#         embedding_dim=192,
#         device=config.device,
#         temperature=config.temperature
#     )
#
#     trainer.optimizer = torch.optim.AdamW(
#         trainer.mlp.parameters(),
#         lr=config.learning_rate,
#         weight_decay=1e-4
#     )
#
#     print(f'âœ… SupCon Trainer initialized')
#     print(f'   - Temperature: {config.temperature}')
#     print(f'   - MLP parameters: {sum(p.numel() for p in trainer.mlp.parameters()):,}')
#     print(f'   - Optimizer: AdamW (lr={config.learning_rate}, wd=1e-4)')
#
#     # [2/3] åŠ è½½æ•°æ®é›†
#     print('\n[2/3] Loading dataset...')
#     train_dataset = VoxCelebMusanDataset(
#         config.voxceleb_dir,
#         config.musan_dir,
#         split='train',
#         snr_range=config.snr_range
#     )
#
#     train_loader = DataLoader(
#         train_dataset,
#         batch_size=config.batch_size,
#         shuffle=True,
#         num_workers=0,
#         collate_fn=pad_collate
#     )
#
#     print(f'âœ… Training samples: {len(train_dataset)}')
#     print(f'   - SNR range: {config.snr_range} dB')
#
#     # [3/3] è®­ç»ƒ
#     print('\n[3/3] Training with SupCon Loss...')
#     print('=' * 80)
#
#     start_time = time.time()
#     total_loss = 0.0
#     loss_history = []
#
#     trainer.mlp.train()
#
#     for batch_idx, audio_batch in enumerate(train_loader, 1):
#         batch_start = time.time()
#
#         # === æå–å¹¶å½’ä¸€åŒ–embeddings ===
#         noisy_embs, enhanced_embs, labels = extract_and_normalize_embeddings(
#             audio_batch, speaker_model, enhancer, config.device
#         )
#
#         # ç§»åˆ°GPU
#         noisy_embs = noisy_embs.to(config.device)
#         enhanced_embs = enhanced_embs.to(config.device)
#         labels = labels.to(config.device)
#
#         # === é€šè¿‡MLPèåˆ ===
#         robust_embs = trainer.mlp(noisy_embs, enhanced_embs)
#
#         # âœ… æ£€æŸ¥å½’ä¸€åŒ–ï¼ˆè°ƒè¯•ç”¨ï¼‰
#         if batch_idx == 1:
#             norms = torch.norm(robust_embs, p=2, dim=1)
#             print(f'\nğŸ” Checking normalization (batch 1):')
#             print(f'   - Robust emb norms: min={norms.min():.4f}, max={norms.max():.4f}, mean={norms.mean():.4f}')
#             if not torch.allclose(norms, torch.ones_like(norms), atol=1e-4):
#                 print(f'   âš ï¸ Warning: Features not perfectly normalized!')
#             else:
#                 print(f'   âœ… Features properly normalized\n')
#
#         # === è®¡ç®—SupConæŸå¤± ===
#         loss = trainer.criterion(robust_embs, labels)
#
#         # === åå‘ä¼ æ’­ ===
#         trainer.optimizer.zero_grad()
#         loss.backward()
#
#         # æ¢¯åº¦è£å‰ªï¼ˆæé«˜ç¨³å®šæ€§ï¼‰
#         torch.nn.utils.clip_grad_norm_(trainer.mlp.parameters(), max_norm=1.0)
#
#         trainer.optimizer.step()
#
#         # === è®°å½• ===
#         loss_val = loss.item()
#         total_loss += loss_val
#         loss_history.append(loss_val)
#
#         batch_time = time.time() - batch_start
#         elapsed_total = (time.time() - start_time) / 60
#
#         # æ‰“å°è¿›åº¦
#         if batch_idx % 5 == 0 or batch_idx == 1:
#             avg_loss = total_loss / batch_idx
#             print(f'Batch {batch_idx:3d}/{config.max_batches} | '
#                   f'Loss: {loss_val:.4f} (avg: {avg_loss:.4f}) | '
#                   f'Time: {batch_time:.1f}s | '
#                   f'Total: {elapsed_total:.1f}min')
#
#         # ä¿å­˜checkpoint
#         if batch_idx % config.checkpoint_interval == 0:
#             checkpoint_path = save_checkpoint(
#                 trainer,
#                 total_loss / batch_idx,
#                 batch_idx,
#                 elapsed_total,
#                 config,
#                 is_final=False
#             )
#             print(f'  ğŸ’¾ Checkpoint saved: {checkpoint_path.name}')
#
#         if batch_idx >= config.max_batches:
#             break
#
#     # === è®­ç»ƒå®Œæˆ ===
#     total_time = (time.time() - start_time) / 60
#     final_loss = total_loss / config.max_batches
#
#     print('\n' + '=' * 80)
#     print('âœ… TRAINING COMPLETED')
#     print('=' * 80)
#     print(f'ğŸ“Š Final loss: {final_loss:.4f}')
#     print(f'â±ï¸  Total time: {total_time:.1f} minutes')
#     print(f'ğŸ“ˆ Avg loss: {sum(loss_history) / len(loss_history):.4f}')
#     print(f'ğŸ“‰ Min loss: {min(loss_history):.4f}')
#     print(f'ğŸ“ˆ Max loss: {max(loss_history):.4f}')
#     print('=' * 80)
#
#     # ä¿å­˜æœ€ç»ˆæ¨¡å‹
#     final_path = save_checkpoint(
#         trainer,
#         final_loss,
#         config.max_batches,
#         total_time,
#         config,
#         is_final=True
#     )
#
#     print(f'\nğŸ’¾ Final model saved: {final_path}')
#
#     # ä¿å­˜losså†å²
#     loss_file = Path(config.checkpoint_dir) / 'loss_history_supcon.txt'
#     with open(loss_file, 'w') as f:
#         f.write('# SupCon Loss History\n')
#         f.write(f'# Temperature: {config.temperature}\n')
#         f.write(f'# Batch_size: {config.batch_size}\n')
#         f.write(f'# Reference: https://github.com/HobbitLong/SupContrast\n')
#         f.write('# Batch\tLoss\n')
#         for i, loss in enumerate(loss_history, 1):
#             f.write(f'{i}\t{loss:.6f}\n')
#     print(f'ğŸ“„ Loss history saved: {loss_file}')
#
#     # åˆ†æè®­ç»ƒç¨³å®šæ€§
#     print('\n' + '=' * 80)
#     print('ğŸ“Š TRAINING STABILITY ANALYSIS')
#     print('=' * 80)
#
#     loss_std = np.std(loss_history)
#     loss_mean = np.mean(loss_history)
#     cv = loss_std / loss_mean
#
#     print(f'Loss Mean: {loss_mean:.4f}')
#     print(f'Loss Std: {loss_std:.4f}')
#     print(f'Coefficient of Variation: {cv:.4f}')
#
#     last_10_avg = np.mean(loss_history[-10:])
#     first_10_avg = np.mean(loss_history[:10])
#     improvement = (first_10_avg - last_10_avg) / first_10_avg * 100
#
#     print(f'\nFirst 10 batches avg loss: {first_10_avg:.4f}')
#     print(f'Last 10 batches avg loss: {last_10_avg:.4f}')
#     print(f'Improvement: {improvement:.2f}%')
#
#     if improvement > 0:
#         print('âœ… Model is converging')
#     else:
#         print('âš ï¸ Model may need more training')
#
#     print('=' * 80)
#
#     # ä¿å­˜åˆ†æç»“æœ
#     analysis_file = Path(config.checkpoint_dir) / 'training_analysis.txt'
#     with open(analysis_file, 'w') as f:
#         f.write('SupCon Training Analysis\n')
#         f.write('=' * 80 + '\n')
#         f.write(f'Loss Mean: {loss_mean:.4f}\n')
#         f.write(f'Loss Std: {loss_std:.4f}\n')
#         f.write(f'CV: {cv:.4f}\n')
#         f.write(f'Improvement: {improvement:.2f}%\n')
#         f.write(f'Total time: {total_time:.1f} min\n')
#         f.write(f'Temperature: {config.temperature}\n')
#         f.write('Reference: https://github.com/HobbitLong/SupContrast\n')
#
#     print(f'\nğŸ“„ Analysis saved: {analysis_file}')
#     print('\nğŸ‰ All done! Next: Compare with Triplet Loss using compare_training.py')
#
#
# if __name__ == '__main__':
#     main()
"""
é˜¶æ®µ1ï¼šSupConæŸå¤±å®Œæ•´å®ç°ï¼ˆä¿®å¤speaker_idå­—ç¬¦ä¸²é—®é¢˜ï¼‰
âœ… å‚è€ƒå®˜æ–¹æºç : https://github.com/HobbitLong/SupContrast
âœ… æ­£ç¡®å¤„ç†å½’ä¸€åŒ–
âœ… ä¿®å¤speaker_idå­—ç¬¦ä¸²->æ•´æ•°æ˜ å°„

ç›®æ ‡ï¼šéªŒè¯è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ï¼Œä¸Triplet Losså¯¹æ¯”
"""

import torch
import torch.nn.functional as F
import os
import subprocess
import sys
import time
import random
import numpy as np
from pathlib import Path
from torch.utils.data import DataLoader

# ç¦ç”¨DeepFilteræ—¥å¿—
os.environ['DF_DISABLE_LOGGING'] = '1'
original_check_output = subprocess.check_output
subprocess.check_output = lambda *args, **kwargs: b'unknown' if args and 'git' in str(
    args[0]) else original_check_output(*args, **kwargs)

sys.path.append('scripts')
from models_supcon import SupConTrainer
from speaker_embedding import SpeakerEmbeddingExtractor
from speech_enhancer import SpeechEnhancer
from dataset import VoxCelebMusanDataset


class Config:
    """SupConè®­ç»ƒé…ç½®"""
    # æ•°æ®è·¯å¾„
    voxceleb_dir = 'data/voxceleb1'
    musan_dir = 'data/musan'

    # è®­ç»ƒå‚æ•°
    batch_size = 32
    learning_rate = 1e-3
    snr_range = (-20, 0)

    # SupConç‰¹æœ‰å‚æ•°
    temperature = 0.07

    # è®­ç»ƒæ§åˆ¶
    max_batches = 200
    checkpoint_interval = 25

    # è¾“å‡ºç›®å½•
    checkpoint_dir = 'checkpoints_supcon_200batch'

    # è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # éšæœºç§å­
    seed = 42


def set_seed(seed):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def pad_collate(batch):
    """
    å¤„ç†å˜é•¿éŸ³é¢‘çš„collateå‡½æ•°

    âœ… ä¿®å¤ç‚¹1ï¼šä¸è½¬æ¢speaker_idä¸ºtensorï¼Œä¿æŒä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
    """
    max_anchor = max([item['anchor_noisy'].shape[1] for item in batch])
    max_positive = max([item['positive_noisy'].shape[1] for item in batch])
    max_negative = max([item['negative_noisy'].shape[1] for item in batch])

    padded_batch = {
        'anchor_noisy': [],
        'positive_noisy': [],
        'negative_noisy': [],
        'snr': [],
        'speaker_id': []
    }

    for item in batch:
        # Anchor padding
        audio = item['anchor_noisy']
        if audio.shape[1] < max_anchor:
            padding = torch.zeros(1, max_anchor - audio.shape[1])
            audio = torch.cat([audio, padding], dim=1)
        padded_batch['anchor_noisy'].append(audio)

        # Positive padding
        audio = item['positive_noisy']
        if audio.shape[1] < max_positive:
            padding = torch.zeros(1, max_positive - audio.shape[1])
            audio = torch.cat([audio, padding], dim=1)
        padded_batch['positive_noisy'].append(audio)

        # Negative padding
        audio = item['negative_noisy']
        if audio.shape[1] < max_negative:
            padding = torch.zeros(1, max_negative - audio.shape[1])
            audio = torch.cat([audio, padding], dim=1)
        padded_batch['negative_noisy'].append(audio)

        padded_batch['snr'].append(item['snr'])
        padded_batch['speaker_id'].append(item['speaker_id'])

    # StackéŸ³é¢‘æ•°æ®
    for key in ['anchor_noisy', 'positive_noisy', 'negative_noisy']:
        padded_batch[key] = torch.stack(padded_batch[key])

    # âœ… å…³é”®ä¿®å¤ï¼šspeaker_idä¿æŒä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¸è½¬æ¢ä¸ºtensor
    # åŸæ¥çš„ä»£ç ï¼špadded_batch['speaker_id'] = torch.tensor(padded_batch['speaker_id'])
    # è¿™ä¼šæŠ¥é”™ï¼šValueError: too many dimensions 'str'

    return padded_batch


def extract_and_normalize_embeddings(audio_batch, speaker_model, enhancer, device, speaker_id_map):
    """
    æå–embeddingså¹¶ç¡®ä¿L2å½’ä¸€åŒ–

    âœ… ä¿®å¤ç‚¹2ï¼šå°†å­—ç¬¦ä¸²speaker_idæ˜ å°„ä¸ºæ•´æ•°

    å‚æ•°:
        speaker_id_map: dict, å­—ç¬¦ä¸²speaker_id -> æ•´æ•°æ˜ å°„

    è¿”å›:
        noisy_embeddings: [batch_size*3, 192]
        enhanced_embeddings: [batch_size*3, 192]
        labels: [batch_size*3] (æ•´æ•°)
    """
    batch_size = len(audio_batch['anchor_noisy'])

    all_noisy_embs = []
    all_enhanced_embs = []
    all_labels = []

    for i in range(batch_size):
        # âœ… ä¿®å¤ç‚¹3ï¼šç›´æ¥è·å–å­—ç¬¦ä¸²ï¼Œä¸è°ƒç”¨.item()
        # åŸæ¥ï¼šspeaker_id = audio_batch['speaker_id'][i].item()  âŒ ä¼šæŠ¥é”™
        # ç°åœ¨ï¼šspeaker_id_str = audio_batch['speaker_id'][i]     âœ… æ­£ç¡®
        speaker_id_str = audio_batch['speaker_id'][i]

        # âœ… ä¿®å¤ç‚¹4ï¼šå°†å­—ç¬¦ä¸²æ˜ å°„ä¸ºæ•´æ•°
        if speaker_id_str not in speaker_id_map:
            speaker_id_map[speaker_id_str] = len(speaker_id_map)
        speaker_id_int = speaker_id_map[speaker_id_str]

        # === å¤„ç†Anchorï¼ˆè¯´è¯äººAï¼‰===
        anchor_noisy = audio_batch['anchor_noisy'][i]
        anchor_enhanced = enhancer.enhance_audio(anchor_noisy, sr=16000)

        emb_noisy = speaker_model.extract_embedding(audio_tensor=anchor_noisy)
        emb_enhanced = speaker_model.extract_embedding(audio_tensor=anchor_enhanced)

        # L2å½’ä¸€åŒ–ï¼ˆSupConè®ºæ–‡è¦æ±‚ï¼‰
        emb_noisy = emb_noisy.squeeze()
        emb_enhanced = emb_enhanced.squeeze()
        emb_noisy = F.normalize(emb_noisy.unsqueeze(0), p=2, dim=1).squeeze(0)
        emb_enhanced = F.normalize(emb_enhanced.unsqueeze(0), p=2, dim=1).squeeze(0)

        all_noisy_embs.append(emb_noisy)
        all_enhanced_embs.append(emb_enhanced)
        all_labels.append(speaker_id_int)

        # === å¤„ç†Positiveï¼ˆè¯´è¯äººAï¼Œä¸åŒutteranceï¼‰===
        pos_noisy = audio_batch['positive_noisy'][i]
        pos_enhanced = enhancer.enhance_audio(pos_noisy, sr=16000)

        emb_noisy = speaker_model.extract_embedding(audio_tensor=pos_noisy)
        emb_enhanced = speaker_model.extract_embedding(audio_tensor=pos_enhanced)

        emb_noisy = F.normalize(emb_noisy.squeeze().unsqueeze(0), p=2, dim=1).squeeze(0)
        emb_enhanced = F.normalize(emb_enhanced.squeeze().unsqueeze(0), p=2, dim=1).squeeze(0)

        all_noisy_embs.append(emb_noisy)
        all_enhanced_embs.append(emb_enhanced)
        all_labels.append(speaker_id_int)  # ä¸anchorç›¸åŒ

        # === å¤„ç†Negativeï¼ˆè¯´è¯äººBï¼Œä¸åŒäººï¼‰===
        neg_noisy = audio_batch['negative_noisy'][i]
        neg_enhanced = enhancer.enhance_audio(neg_noisy, sr=16000)

        emb_noisy = speaker_model.extract_embedding(audio_tensor=neg_noisy)
        emb_enhanced = speaker_model.extract_embedding(audio_tensor=neg_enhanced)

        emb_noisy = F.normalize(emb_noisy.squeeze().unsqueeze(0), p=2, dim=1).squeeze(0)
        emb_enhanced = F.normalize(emb_enhanced.squeeze().unsqueeze(0), p=2, dim=1).squeeze(0)

        all_noisy_embs.append(emb_noisy)
        all_enhanced_embs.append(emb_enhanced)
        # Negativeçš„æ ‡ç­¾åº”è¯¥ä¸åŒï¼Œç”¨å¤§åç§»é‡ç¡®ä¿ä¸å†²çª
        all_labels.append(speaker_id_int + 100000)

    # å †å æˆbatch
    noisy_embeddings = torch.stack(all_noisy_embs)
    enhanced_embeddings = torch.stack(all_enhanced_embs)
    labels = torch.tensor(all_labels, dtype=torch.long)

    return noisy_embeddings, enhanced_embeddings, labels


def save_checkpoint(trainer, loss, batch_idx, total_time, config, is_final=False):
    """ä¿å­˜checkpoint"""
    checkpoint_dir = Path(config.checkpoint_dir)
    checkpoint_dir.mkdir(exist_ok=True)

    if is_final:
        filename = 'final_model_supcon.pth'
    else:
        filename = f'checkpoint_supcon_batch_{batch_idx}.pth'

    checkpoint_path = checkpoint_dir / filename

    torch.save({
        'model_state_dict': trainer.mlp.state_dict(),
        'optimizer_state_dict': trainer.optimizer.state_dict(),
        'loss': loss,
        'batch': batch_idx,
        'training_minutes': total_time,
        'config': {
            'loss_type': 'SupCon',
            'batch_size': config.batch_size,
            'learning_rate': config.learning_rate,
            'temperature': config.temperature,
            'optimizer': 'AdamW',
            'snr_range': config.snr_range,
            'max_batches': config.max_batches,
            'seed': config.seed
        }
    }, checkpoint_path)

    return checkpoint_path


def main():
    config = Config()
    set_seed(config.seed)

    print('=' * 80)
    print('ğŸš€ SUPCON TRAINING - Supervised Contrastive Learning')
    print('=' * 80)
    print(f'ğŸ“Œ Loss Function: SupCon (replacing Triplet Loss)')
    print(f'ğŸ¯ Goal: Verify training stability and convergence')
    print(f'ğŸ“š Reference: Supervised Contrastive Learning (NeurIPS 2020)')
    print(f'ğŸ’» Official Code: https://github.com/HobbitLong/SupContrast')
    print('=' * 80)
    print(f'Device: {config.device}')
    print(f'Random seed: {config.seed}')
    print(f'Batch size: {config.batch_size}')
    print(f'Learning rate: {config.learning_rate}')
    print(f'Temperature: {config.temperature} ğŸŒ¡ï¸')
    print(f'Max batches: {config.max_batches}')
    print('=' * 80)

    # [1/3] åŠ è½½æ¨¡å‹
    print('\n[1/3] Loading models...')
    speaker_model = SpeakerEmbeddingExtractor('ecapa')
    enhancer = SpeechEnhancer()

    trainer = SupConTrainer(
        embedding_dim=192,
        device=config.device,
        temperature=config.temperature
    )

    trainer.optimizer = torch.optim.AdamW(
        trainer.mlp.parameters(),
        lr=config.learning_rate,
        weight_decay=1e-4
    )

    print(f'âœ… SupCon Trainer initialized')
    print(f'   - Temperature: {config.temperature}')
    print(f'   - MLP parameters: {sum(p.numel() for p in trainer.mlp.parameters()):,}')
    print(f'   - Optimizer: AdamW (lr={config.learning_rate}, wd=1e-4)')

    # [2/3] åŠ è½½æ•°æ®é›†
    print('\n[2/3] Loading dataset...')
    train_dataset = VoxCelebMusanDataset(
        config.voxceleb_dir,
        config.musan_dir,
        split='train',
        snr_range=config.snr_range
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0,
        collate_fn=pad_collate
    )

    print(f'âœ… Training samples: {len(train_dataset)}')
    print(f'   - SNR range: {config.snr_range} dB')

    # [3/3] è®­ç»ƒ
    print('\n[3/3] Training with SupCon Loss...')
    print('=' * 80)

    start_time = time.time()
    total_loss = 0.0
    loss_history = []

    # âœ… ä¿®å¤ç‚¹5ï¼šåˆ›å»ºspeaker_idæ˜ å°„å­—å…¸
    speaker_id_map = {}

    trainer.mlp.train()

    for batch_idx, audio_batch in enumerate(train_loader, 1):
        batch_start = time.time()

        # âœ… ä¿®å¤ç‚¹6ï¼šä¼ é€’speaker_id_map
        noisy_embs, enhanced_embs, labels = extract_and_normalize_embeddings(
            audio_batch, speaker_model, enhancer, config.device, speaker_id_map
        )

        # ç§»åˆ°GPU
        noisy_embs = noisy_embs.to(config.device)
        enhanced_embs = enhanced_embs.to(config.device)
        labels = labels.to(config.device)

        # é€šè¿‡MLPèåˆ
        robust_embs = trainer.mlp(noisy_embs, enhanced_embs)

        # æ£€æŸ¥å½’ä¸€åŒ–ï¼ˆç¬¬ä¸€ä¸ªbatchï¼‰
        if batch_idx == 1:
            norms = torch.norm(robust_embs, p=2, dim=1)
            print(f'\nğŸ” Checking normalization (batch 1):')
            print(f'   - Robust emb norms: min={norms.min():.4f}, max={norms.max():.4f}, mean={norms.mean():.4f}')
            if not torch.allclose(norms, torch.ones_like(norms), atol=1e-4):
                print(f'   âš ï¸ Warning: Features not perfectly normalized!')
            else:
                print(f'   âœ… Features properly normalized')
            print(f'   - Unique speakers in batch: {len(speaker_id_map)}')
            print()

        # è®¡ç®—SupConæŸå¤±
        loss = trainer.criterion(robust_embs, labels)

        # åå‘ä¼ æ’­
        trainer.optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(trainer.mlp.parameters(), max_norm=1.0)

        trainer.optimizer.step()

        # è®°å½•
        loss_val = loss.item()
        total_loss += loss_val
        loss_history.append(loss_val)

        batch_time = time.time() - batch_start
        elapsed_total = (time.time() - start_time) / 60

        # æ‰“å°è¿›åº¦
        if batch_idx % 5 == 0 or batch_idx == 1:
            avg_loss = total_loss / batch_idx
            print(f'Batch {batch_idx:3d}/{config.max_batches} | '
                  f'Loss: {loss_val:.4f} (avg: {avg_loss:.4f}) | '
                  f'Time: {batch_time:.1f}s | '
                  f'Total: {elapsed_total:.1f}min')

        # ä¿å­˜checkpoint
        if batch_idx % config.checkpoint_interval == 0:
            checkpoint_path = save_checkpoint(
                trainer,
                total_loss / batch_idx,
                batch_idx,
                elapsed_total,
                config,
                is_final=False
            )
            print(f'  ğŸ’¾ Checkpoint saved: {checkpoint_path.name}')

        if batch_idx >= config.max_batches:
            break

    # è®­ç»ƒå®Œæˆ
    total_time = (time.time() - start_time) / 60
    final_loss = total_loss / config.max_batches

    print('\n' + '=' * 80)
    print('âœ… TRAINING COMPLETED')
    print('=' * 80)
    print(f'ğŸ“Š Final loss: {final_loss:.4f}')
    print(f'â±ï¸  Total time: {total_time:.1f} minutes')
    print(f'ğŸ“ˆ Avg loss: {sum(loss_history) / len(loss_history):.4f}')
    print(f'ğŸ“‰ Min loss: {min(loss_history):.4f}')
    print(f'ğŸ“ˆ Max loss: {max(loss_history):.4f}')
    print(f'ğŸ‘¥ Total unique speakers: {len(speaker_id_map)}')
    print('=' * 80)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = save_checkpoint(
        trainer,
        final_loss,
        config.max_batches,
        total_time,
        config,
        is_final=True
    )

    print(f'\nğŸ’¾ Final model saved: {final_path}')

    # ä¿å­˜losså†å²
    loss_file = Path(config.checkpoint_dir) / 'loss_history_supcon.txt'
    with open(loss_file, 'w') as f:
        f.write('# SupCon Loss History\n')
        f.write(f'# Temperature: {config.temperature}\n')
        f.write(f'# Batch_size: {config.batch_size}\n')
        f.write(f'# Total_speakers: {len(speaker_id_map)}\n')
        f.write(f'# Reference: https://github.com/HobbitLong/SupContrast\n')
        f.write('# Batch\tLoss\n')
        for i, loss in enumerate(loss_history, 1):
            f.write(f'{i}\t{loss:.6f}\n')
    print(f'ğŸ“„ Loss history saved: {loss_file}')

    # åˆ†æè®­ç»ƒç¨³å®šæ€§
    print('\n' + '=' * 80)
    print('ğŸ“Š TRAINING STABILITY ANALYSIS')
    print('=' * 80)

    loss_std = np.std(loss_history)
    loss_mean = np.mean(loss_history)
    cv = loss_std / loss_mean

    print(f'Loss Mean: {loss_mean:.4f}')
    print(f'Loss Std: {loss_std:.4f}')
    print(f'Coefficient of Variation: {cv:.4f}')

    last_10_avg = np.mean(loss_history[-10:])
    first_10_avg = np.mean(loss_history[:10])
    improvement = (first_10_avg - last_10_avg) / first_10_avg * 100

    print(f'\nFirst 10 batches avg loss: {first_10_avg:.4f}')
    print(f'Last 10 batches avg loss: {last_10_avg:.4f}')
    print(f'Improvement: {improvement:.2f}%')

    if improvement > 0:
        print('âœ… Model is converging')
    else:
        print('âš ï¸ Model may need more training')

    print('=' * 80)

    # ä¿å­˜åˆ†æç»“æœ
    analysis_file = Path(config.checkpoint_dir) / 'training_analysis.txt'
    with open(analysis_file, 'w') as f:
        f.write('SupCon Training Analysis\n')
        f.write('=' * 80 + '\n')
        f.write(f'Loss Mean: {loss_mean:.4f}\n')
        f.write(f'Loss Std: {loss_std:.4f}\n')
        f.write(f'CV: {cv:.4f}\n')
        f.write(f'Improvement: {improvement:.2f}%\n')
        f.write(f'Total time: {total_time:.1f} min\n')
        f.write(f'Temperature: {config.temperature}\n')
        f.write(f'Unique speakers: {len(speaker_id_map)}\n')
        f.write('Reference: https://github.com/HobbitLong/SupContrast\n')

    print(f'\nğŸ“„ Analysis saved: {analysis_file}')
    print('\nğŸ‰ All done! Next: Compare with Triplet Loss using compare_training.py')


if __name__ == '__main__':
    main()