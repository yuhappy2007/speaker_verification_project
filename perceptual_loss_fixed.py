"""
WavLMæ„ŸçŸ¥æŸå¤±æ¨¡å— (ä¿®å¤ç‰ˆ)
- ä½¿ç”¨é¢„è®­ç»ƒWavLMæå–ç‰¹å¾
- è®¡ç®—å¢å¼ºè¯­éŸ³ä¸å¹²å‡€è¯­éŸ³çš„æ„ŸçŸ¥è·ç¦»
- å‚æ•°å†»ç»“ï¼Œä»…ç”¨äºæŸå¤±è®¡ç®—
- é€‚é…GTCRNè®­ç»ƒæµç¨‹

ä¿®å¤ï¼š
1. ä¿®å¤ Wav2Vec2FeatureExtractor å¯¼å…¥
2. æ·»åŠ é…ç½®æ–‡ä»¶æ”¯æŒ
3. ä¼˜åŒ–æ¥å£ä»¥é€‚é…è®­ç»ƒæµç¨‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import WavLMModel, Wav2Vec2FeatureExtractor  # âœ… ä¿®å¤ï¼šæ·»åŠ  Wav2Vec2FeatureExtractor
from pathlib import Path


class WavLMPerceptualLoss(nn.Module):
    """
    åŸºäºWavLMçš„æ„ŸçŸ¥æŸå¤±

    å‚è€ƒ:
    - WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing
    - Perceptual loss for speech enhancement

    åŠŸèƒ½:
    1. å†»ç»“çš„WavLMæ¨¡å‹æå–éŸ³é¢‘ç‰¹å¾
    2. è®¡ç®—å¢å¼ºè¯­éŸ³å’Œå¹²å‡€è¯­éŸ³ç‰¹å¾çš„è·ç¦»
    3. æ”¯æŒå¤šå±‚ç‰¹å¾èåˆ
    """

    def __init__(self,
                 model_path='D:/WavLM',  # æœ¬åœ°WavLMè·¯å¾„
                 feature_layers=[3, 7, 11],  # ä½¿ç”¨å“ªäº›å±‚çš„ç‰¹å¾
                 loss_type='l1',  # 'l1', 'l2', 'cosine'
                 normalize=True,  # æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾
                 device='cuda'):
        """
        Args:
            model_path: WavLMæ¨¡å‹è·¯å¾„
            feature_layers: æå–ç‰¹å¾çš„å±‚ç´¢å¼•ï¼ˆWavLM-Baseæœ‰12å±‚ï¼‰
            loss_type: æŸå¤±ç±»å‹
            normalize: æ˜¯å¦å¯¹ç‰¹å¾è¿›è¡ŒL2å½’ä¸€åŒ–
            device: è®¡ç®—è®¾å¤‡
        """
        super().__init__()

        self.device = device
        self.feature_layers = feature_layers
        self.loss_type = loss_type
        self.normalize = normalize

        print(f'ğŸµ Initializing WavLM Perceptual Loss...')
        print(f'   Model path: {model_path}')
        print(f'   Feature layers: {feature_layers}')
        print(f'   Loss type: {loss_type}')
        print(f'   Device: {device}')

        # åŠ è½½WavLMæ¨¡å‹
        try:
            self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_path)
            self.model = WavLMModel.from_pretrained(model_path)
            self.model = self.model.to(device)
            self.model.eval()

            # å†»ç»“æ‰€æœ‰å‚æ•°
            for param in self.model.parameters():
                param.requires_grad = False

            print(f'âœ… WavLM model loaded and frozen')

            # è·å–æ¨¡å‹é…ç½®
            self.sampling_rate = self.feature_extractor.sampling_rate
            print(f'   Expected sampling rate: {self.sampling_rate} Hz')

        except Exception as e:
            print(f'âŒ Failed to load WavLM model: {e}')
            raise

        # å±‚æƒé‡ï¼ˆå¯é€‰ï¼šä¸åŒå±‚çš„é‡è¦æ€§ä¸åŒï¼‰
        # âœ… ä¿®å¤ï¼šç›´æ¥åœ¨ç›®æ ‡è®¾å¤‡ä¸Šåˆ›å»º
        self.layer_weights = nn.Parameter(
            torch.ones(len(feature_layers), device=device) / len(feature_layers),
            requires_grad=False  # ä¹Ÿå†»ç»“
        )

    def extract_features(self, audio):
        """
        æå–WavLMç‰¹å¾

        Args:
            audio: [batch, samples] æˆ– [batch, 1, samples]

        Returns:
            features: dict of {layer_idx: [batch, time, hidden_dim]}
        """
        # ç¡®ä¿æ˜¯2D: [batch, samples]
        if audio.dim() == 3:
            audio = audio.squeeze(1)

        batch_size = audio.shape[0]

        # WavLMæœŸæœ›è¾“å…¥ä¸º16kHz
        # æ³¨æ„ï¼šç¡®ä¿è¾“å…¥å·²ç»æ˜¯16kHz

        # âœ… ä¿®å¤ï¼šç§»é™¤ with torch.no_grad()
        # WavLMå‚æ•°å·²è¢«å†»ç»“ï¼ˆrequires_grad=Falseï¼‰ï¼Œä½†æˆ‘ä»¬éœ€è¦ä¿æŒæ¢¯åº¦æµå‘è¾“å…¥
        # è¿™æ ·æ‰èƒ½åœ¨è®­ç»ƒGTCRNæ—¶é€šè¿‡WavLMåå‘ä¼ æ’­æ¢¯åº¦

        # WavLMçš„forwardè¿”å›æ‰€æœ‰å±‚çš„hidden states
        outputs = self.model(
            audio,
            output_hidden_states=True,
            return_dict=True
        )

        hidden_states = outputs.hidden_states  # Tuple of [batch, time, 768]

        # æå–æŒ‡å®šå±‚çš„ç‰¹å¾
        features = {}
        for layer_idx in self.feature_layers:
            feat = hidden_states[layer_idx]  # [batch, time, 768]

            # å¯é€‰ï¼šL2å½’ä¸€åŒ–
            if self.normalize:
                feat = F.normalize(feat, p=2, dim=-1)

            features[layer_idx] = feat

        return features

    def compute_loss(self, enhanced_audio, clean_audio):
        """
        è®¡ç®—æ„ŸçŸ¥æŸå¤±

        Args:
            enhanced_audio: [batch, samples] æˆ– [batch, 1, samples]ï¼Œå¢å¼ºåçš„è¯­éŸ³
            clean_audio: [batch, samples] æˆ– [batch, 1, samples]ï¼Œå¹²å‡€ç›®æ ‡è¯­éŸ³

        Returns:
            loss: scalarï¼Œæ„ŸçŸ¥æŸå¤±
            loss_dict: dictï¼Œå„å±‚æŸå¤±çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºç›‘æ§ï¼‰
        """
        # æå–ç‰¹å¾
        enhanced_features = self.extract_features(enhanced_audio)
        clean_features = self.extract_features(clean_audio)

        # è®¡ç®—å„å±‚æŸå¤±
        layer_losses = []
        loss_dict = {}

        for i, layer_idx in enumerate(self.feature_layers):
            enhanced_feat = enhanced_features[layer_idx]  # [batch, time, dim]
            clean_feat = clean_features[layer_idx]

            # è®¡ç®—è·ç¦»
            if self.loss_type == 'l1':
                layer_loss = F.l1_loss(enhanced_feat, clean_feat)
            elif self.loss_type == 'l2':
                layer_loss = F.mse_loss(enhanced_feat, clean_feat)
            elif self.loss_type == 'cosine':
                # Cosine similarity loss
                cos_sim = F.cosine_similarity(
                    enhanced_feat.flatten(1),
                    clean_feat.flatten(1),
                    dim=1
                ).mean()
                layer_loss = 1 - cos_sim
            else:
                raise ValueError(f'Unknown loss type: {self.loss_type}')

            layer_losses.append(layer_loss)
            loss_dict[f'layer_{layer_idx}'] = layer_loss.item()

        # åŠ æƒæ±‚å’Œ
        layer_losses = torch.stack(layer_losses)
        total_loss = (layer_losses * self.layer_weights).sum()

        loss_dict['total'] = total_loss.item()

        return total_loss, loss_dict

    def forward(self, enhanced_audio, clean_audio):
        """å‰å‘ä¼ æ’­ï¼ˆè°ƒç”¨compute_lossï¼‰"""
        return self.compute_loss(enhanced_audio, clean_audio)


def test_perceptual_loss():
    """æµ‹è¯•æ„ŸçŸ¥æŸå¤±æ¨¡å—"""
    print('=' * 80)
    print('ğŸ§ª Testing WavLM Perceptual Loss')
    print('=' * 80)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # åˆå§‹åŒ–
    perceptual_loss = WavLMPerceptualLoss(
        model_path='D:/WavLM',
        feature_layers=[3, 7, 11],
        loss_type='l1',
        device=device
    )

    # æµ‹è¯•æ•°æ®ï¼ˆ16kHzï¼Œ3ç§’ï¼‰
    batch_size = 2
    duration = 3
    sr = 16000

    print(f'\nğŸ“Š Test configuration:')
    print(f'   Batch size: {batch_size}')
    print(f'   Duration: {duration}s')
    print(f'   Sampling rate: {sr} Hz')

    # æ¨¡æ‹Ÿå¢å¼ºè¯­éŸ³å’Œå¹²å‡€è¯­éŸ³
    enhanced = torch.randn(batch_size, sr * duration).to(device)
    clean = torch.randn(batch_size, sr * duration).to(device)

    print(f'\nğŸ”Š Audio shapes:')
    print(f'   Enhanced: {enhanced.shape}')
    print(f'   Clean: {clean.shape}')

    # è®¡ç®—æŸå¤±
    print(f'\nğŸ§® Computing perceptual loss...')
    loss, loss_dict = perceptual_loss(enhanced, clean)

    print(f'\nğŸ“ˆ Loss results:')
    print(f'   Total loss: {loss.item():.6f}')
    for layer, value in loss_dict.items():
        if layer != 'total':
            print(f'   {layer}: {value:.6f}')

    # æµ‹è¯•æ¢¯åº¦æµï¼ˆæ¨¡æ‹Ÿå®é™…è®­ç»ƒåœºæ™¯ï¼‰
    print(f'\nğŸ”„ Testing gradient flow...')

    # æ¨¡æ‹Ÿ GTCRN è¾“å‡ºï¼šåˆ›å»ºä¸€ä¸ªéœ€è¦æ¢¯åº¦çš„"æ¨¡å‹"
    # åœ¨å®é™…è®­ç»ƒä¸­ï¼Œè¿™ç›¸å½“äº GTCRN çš„å‚æ•°
    mock_gtcrn_weight = nn.Parameter(torch.randn(1, sr * duration).to(device))

    # æ¨¡æ‹Ÿ GTCRN çš„è¾“å‡ºï¼ˆéå¶å­å¼ é‡ï¼‰
    enhanced_with_grad = mock_gtcrn_weight * torch.randn(batch_size, sr * duration).to(device)
    clean_no_grad = torch.randn(batch_size, sr * duration).to(device)

    # è®¡ç®—æŸå¤±
    loss, _ = perceptual_loss(enhanced_with_grad, clean_no_grad)
    loss.backward()

    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æµåˆ°"æ¨¡å‹å‚æ•°"ï¼ˆå¶å­å¼ é‡ï¼‰
    print(f'   Mock GTCRN weight has gradient: {mock_gtcrn_weight.grad is not None}')
    if mock_gtcrn_weight.grad is not None:
        print(f'   Gradient norm: {mock_gtcrn_weight.grad.norm().item():.6f}')
        print(f'   âœ… Gradient flows correctly to model parameters!')
    else:
        print(f'   âŒ No gradient! Something is wrong.')

    # æ£€æŸ¥ enhanced æœ¬èº«çš„ grad_fnï¼ˆè¯æ˜å®ƒåœ¨è®¡ç®—å›¾ä¸­ï¼‰
    print(f'   Enhanced has grad_fn: {enhanced_with_grad.grad_fn is not None}')
    print(f'   Enhanced is leaf: {enhanced_with_grad.is_leaf}')  # åº”è¯¥æ˜¯ False

    # æ£€æŸ¥WavLMå‚æ•°æ˜¯å¦è¢«å†»ç»“
    print(f'\nğŸ”’ Checking frozen parameters...')
    frozen_count = sum(1 for p in perceptual_loss.model.parameters() if not p.requires_grad)
    total_count = sum(1 for p in perceptual_loss.model.parameters())
    print(f'   Frozen parameters: {frozen_count}/{total_count}')

    print('\n' + '=' * 80)
    print('âœ… All tests passed!')
    print('=' * 80)


if __name__ == '__main__':
    test_perceptual_loss()