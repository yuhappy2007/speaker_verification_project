"""
å°è§„æ¨¡æµ‹è¯•è„šæœ¬ - ç¬”è®°æœ¬ç‰ˆï¼ˆä¿®å¤ç‰ˆï¼‰
å¿«é€ŸéªŒè¯è®­ç»ƒæµç¨‹ï¼Œåªè®­ç»ƒå‡ ä¸ªbatch
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / 'scripts'))

from gtcrn_wrapper_fixed import GTCRNWrapper
from ecapa_tdnn_wrapper import ECAPATDNNWrapper
from contrastive_loss import ContrastiveLoss
from dataset_fixed import VoxCelebMusanDataset
from fixed_snr_dataset import FixedSNRDataset, collate_fn_fixed_length


def custom_collate_fn(batch):
    """å…¨å±€ collate å‡½æ•°"""
    return collate_fn_fixed_length(batch, target_length=48000)


class ConfidenceNetwork(nn.Module):
    """ç½®ä¿¡åº¦ç½‘ç»œï¼ˆç®€åŒ–ç‰ˆï¼‰"""

    def __init__(self, embedding_dim=192, hidden_dim=256):
        super().__init__()

        self.fusion = nn.Sequential(
            nn.Linear(embedding_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, embedding_dim)
        )

        self.attention = nn.Sequential(
            nn.Linear(embedding_dim * 2, 64),
            nn.ReLU(),
            nn.Linear(64, 2),
            nn.Softmax(dim=1)
        )

    def forward(self, emb_noisy, emb_enhanced):
        concat = torch.cat([emb_noisy, emb_enhanced], dim=1)
        weights = self.attention(concat)
        w_noisy = weights[:, 0:1]
        w_enhanced = weights[:, 1:2]

        weighted = w_noisy * emb_noisy + w_enhanced * emb_enhanced
        fused = self.fusion(concat)
        output = weighted + fused
        output = nn.functional.normalize(output, p=2, dim=1)

        return output


def test_mini_training():
    """
    è¿·ä½ è®­ç»ƒæµ‹è¯•
    - åªç”¨å°‘é‡æ ·æœ¬
    - åªè®­ç»ƒ1-2ä¸ªepoch
    - å¿«é€ŸéªŒè¯æ•´ä¸ªæµç¨‹
    """

    print("\n" + "=" * 80)
    print("Mini Training Test (Notebook Version)")
    print("=" * 80)

    # é…ç½®
    VOXCELEB_DIR = 'data/voxceleb1'
    MUSAN_DIR = 'data/musan'
    GTCRN_CHECKPOINT = 'checkpoints/gtcrn/gtcrn_best.pth'
    SPEAKER_MODEL = 'pretrained_models/spkrec-ecapa-voxceleb'

    NUM_TRAIN_SAMPLES = 32  # åªç”¨32ä¸ªæ ·æœ¬
    NUM_VAL_SAMPLES = 8
    BATCH_SIZE = 4
    NUM_EPOCHS = 2

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nDevice: {device}")

    # ========== 1. åŠ è½½æ•°æ®é›† ==========
    print("\n1. Loading datasets (mini version)...")
    try:
        # å®Œæ•´æ•°æ®é›†
        full_train_dataset = VoxCelebMusanDataset(
            voxceleb_dir=VOXCELEB_DIR,
            musan_dir=MUSAN_DIR,
            split='train',
            snr_range=(-5, 15),
            return_clean=False
        )

        full_val_dataset = VoxCelebMusanDataset(
            voxceleb_dir=VOXCELEB_DIR,
            musan_dir=MUSAN_DIR,
            split='test',
            test_snr=0,
            test_noise_type='noise',
            return_clean=False
        )

        # âœ… ä¿®å¤ï¼šç›´æ¥ä»å®Œæ•´æ•°æ®é›†åˆ›å»ºå­é›†ï¼Œç„¶ååŒ…è£…æˆ FixedSNRDataset
        # ä¸è¦ç”¨ Subsetï¼Œå› ä¸º FixedSNRDataset éœ€è¦è®¿é—® snr_range ç­‰å±æ€§
        import random

        # åˆ›å»ºä¸€ä¸ªå°çš„è®­ç»ƒæ•°æ®é›†ï¼ˆä½¿ç”¨å‰ NUM_TRAIN_SAMPLES ä¸ªæ ·æœ¬ï¼‰
        # è¿™æ ·é¿å… Subset çš„é—®é¢˜
        class SmallDataset:
            """ç®€å•çš„æ•°æ®é›†åŒ…è£…å™¨"""

            def __init__(self, base_dataset, num_samples):
                self.base_dataset = base_dataset
                self.indices = random.sample(range(len(base_dataset)), min(num_samples, len(base_dataset)))
                # å¤åˆ¶å¿…è¦çš„å±æ€§
                self.snr_range = base_dataset.snr_range
                self.return_clean = base_dataset.return_clean

            def __len__(self):
                return len(self.indices)

            def __getitem__(self, idx):
                return self.base_dataset[self.indices[idx]]

        small_train_dataset = SmallDataset(full_train_dataset, NUM_TRAIN_SAMPLES)
        small_val_dataset = SmallDataset(full_val_dataset, NUM_VAL_SAMPLES)

        # åŒ…è£…æˆ FixedSNRDataset
        train_dataset = FixedSNRDataset(small_train_dataset, snr_values=[-5, 0, 5])

        print(f"   âœ… Train samples: {len(train_dataset)}")
        print(f"   âœ… Val samples: {len(small_val_dataset)}")

    except Exception as e:
        print(f"   âŒ Failed to load datasets: {e}")
        import traceback
        traceback.print_exc()
        return

    # ========== 2. DataLoader ==========
    print("\n2. Creating DataLoaders...")
    try:
        train_loader = DataLoader(
            train_dataset,
            batch_size=BATCH_SIZE,
            shuffle=True,
            num_workers=0,  # ç¬”è®°æœ¬ä¸Šç”¨å•çº¿ç¨‹
            collate_fn=custom_collate_fn
        )

        val_loader = DataLoader(
            small_val_dataset,  # æ³¨æ„ï¼šéªŒè¯é›†ä¸ç”¨ FixedSNRDataset åŒ…è£…
            batch_size=BATCH_SIZE,
            shuffle=False,
            num_workers=0,
            collate_fn=custom_collate_fn
        )

        print(f"   âœ… Train batches: {len(train_loader)}")
        print(f"   âœ… Val batches: {len(val_loader)}")

    except Exception as e:
        print(f"   âŒ Failed to create DataLoaders: {e}")
        return

    # ========== 3. åŠ è½½æ¨¡å‹ ==========
    print("\n3. Loading models...")

    # GTCRN
    print("   Loading GTCRN...")
    try:
        gtcrn = GTCRNWrapper(
            checkpoint_path=GTCRN_CHECKPOINT,
            device=device,
            freeze=True
        )
        print("   âœ… GTCRN loaded")
    except Exception as e:
        print(f"   âŒ Failed to load GTCRN: {e}")
        return

    # ECAPA-TDNN
    print("   Loading ECAPA-TDNN...")
    try:
        speaker_model = ECAPATDNNWrapper(
            model_path=SPEAKER_MODEL,
            device=device,
            freeze=True
        )
        print("   âœ… ECAPA-TDNN loaded")
    except Exception as e:
        print(f"   âŒ Failed to load ECAPA-TDNN: {e}")
        return

    # Confidence Network
    print("   Initializing Confidence Network...")
    confidence_net = ConfidenceNetwork(embedding_dim=192, hidden_dim=256).to(device)
    print("   âœ… Confidence Network initialized")

    # ========== 4. è®­ç»ƒè®¾ç½® ==========
    criterion = ContrastiveLoss(temperature=0.07)
    optimizer = optim.AdamW(confidence_net.parameters(), lr=1e-3)

    print("\n4. Training setup complete")

    # ========== 5. è®­ç»ƒå¾ªç¯ ==========
    print(f"\n5. Starting mini training ({NUM_EPOCHS} epochs)...")
    print("=" * 80)

    for epoch in range(1, NUM_EPOCHS + 1):
        print(f"\nEpoch {epoch}/{NUM_EPOCHS}")
        print("-" * 80)

        # è®­ç»ƒ
        confidence_net.train()
        total_loss = 0
        num_batches = 0

        for batch_idx, batch in enumerate(train_loader):
            try:
                # æ•°æ®
                noisy = batch['anchor_noisy'].to(device)
                speaker_ids = batch['speaker_id']

                # æ ‡ç­¾
                unique_speakers = list(set(speaker_ids))
                speaker_to_idx = {spk: idx for idx, spk in enumerate(unique_speakers)}
                labels = torch.tensor([speaker_to_idx[spk] for spk in speaker_ids]).to(device)

                # å‰å‘ä¼ æ’­
                with torch.no_grad():
                    enhanced = gtcrn.enhance(noisy)
                    emb_noisy = speaker_model(noisy)
                    emb_enhanced = speaker_model(enhanced)

                emb_fused = confidence_net(emb_noisy, emb_enhanced)
                loss = criterion(emb_fused, labels)

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                num_batches += 1

                print(f"  Batch {batch_idx + 1}/{len(train_loader)}: loss={loss.item():.4f}")

            except Exception as e:
                print(f"  âŒ Error in batch {batch_idx}: {e}")
                import traceback
                traceback.print_exc()
                continue

        if num_batches > 0:
            avg_loss = total_loss / num_batches
            print(f"\nEpoch {epoch} - Train Loss: {avg_loss:.4f}")
        else:
            print(f"\nâŒ No batches processed in epoch {epoch}")

        # éªŒè¯
        print("\nValidation...")
        confidence_net.eval()
        val_loss = 0
        val_batches = 0

        with torch.no_grad():
            for batch in val_loader:
                try:
                    noisy = batch['anchor_noisy'].to(device)
                    speaker_ids = batch['speaker_id']

                    unique_speakers = list(set(speaker_ids))
                    speaker_to_idx = {spk: idx for idx, spk in enumerate(unique_speakers)}
                    labels = torch.tensor([speaker_to_idx[spk] for spk in speaker_ids]).to(device)

                    enhanced = gtcrn.enhance(noisy)
                    emb_noisy = speaker_model(noisy)
                    emb_enhanced = speaker_model(enhanced)
                    emb_fused = confidence_net(emb_noisy, emb_enhanced)

                    loss = criterion(emb_fused, labels)
                    val_loss += loss.item()
                    val_batches += 1

                except Exception as e:
                    print(f"  âŒ Validation error: {e}")
                    continue

        if val_batches > 0:
            avg_val_loss = val_loss / val_batches
            print(f"Epoch {epoch} - Val Loss: {avg_val_loss:.4f}")
        else:
            print(f"âŒ No validation batches processed")

    print("\n" + "=" * 80)
    print("âœ… Mini training test completed!")
    print("=" * 80)
    print("\nNext steps:")
    print("  1. âœ… Models loaded successfully")
    print("  2. âœ… Training loop works")
    print("  3. ğŸ“¤ Upload to server: scp ecapa_tdnn_wrapper.py server:/path/")
    print("  4. ğŸš€ Run full training on server")


if __name__ == '__main__':
    test_mini_training()