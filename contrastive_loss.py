# """
# å¯¹æ¯”æŸå¤±å‡½æ•° (Contrastive Loss)
# ç”¨äºç½®ä¿¡åº¦ç½‘ç»œè®­ç»ƒ
# """
#
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
#
#
# class ContrastiveLoss(nn.Module):
#     """
#     å¯¹æ¯”æŸå¤±å‡½æ•°
#
#     ç”¨é€”: è®­ç»ƒç½®ä¿¡åº¦ç½‘ç»œï¼Œä½¿åŒä¸€è¯´è¯äººçš„åµŒå…¥è·ç¦»è¿‘ï¼Œä¸åŒè¯´è¯äººçš„åµŒå…¥è·ç¦»è¿œ
#
#     å…¬å¼:
#     - æ­£æ ·æœ¬å¯¹: L_pos = d(x1, x2)^2
#     - è´Ÿæ ·æœ¬å¯¹: L_neg = max(0, margin - d(x1, x2))^2
#     - æ€»æŸå¤±: L = (1-y) * L_pos + y * L_neg
#
#     å…¶ä¸­:
#     - y=0è¡¨ç¤ºåŒä¸€è¯´è¯äºº(æ­£æ ·æœ¬å¯¹)
#     - y=1è¡¨ç¤ºä¸åŒè¯´è¯äºº(è´Ÿæ ·æœ¬å¯¹)
#     - marginæ˜¯é—´éš”è¶…å‚æ•°
#     """
#
#     def __init__(self, margin=1.0, distance_type='euclidean'):
#         """
#         Args:
#             margin: è´Ÿæ ·æœ¬å¯¹çš„æœ€å°è·ç¦»é—´éš”
#             distance_type: 'euclidean' æˆ– 'cosine'
#         """
#         super().__init__()
#         self.margin = margin
#         self.distance_type = distance_type
#
#         print(f'ğŸ“ Contrastive Loss initialized')
#         print(f'   Margin: {margin}')
#         print(f'   Distance: {distance_type}')
#
#     def compute_distance(self, x1, x2):
#         """
#         è®¡ç®—ä¸¤ä¸ªåµŒå…¥ä¹‹é—´çš„è·ç¦»
#
#         Args:
#             x1, x2: [batch, embedding_dim]
#
#         Returns:
#             distance: [batch]
#         """
#         if self.distance_type == 'euclidean':
#             # æ¬§æ°è·ç¦»
#             distance = torch.sqrt(torch.sum((x1 - x2) ** 2, dim=1) + 1e-8)
#         elif self.distance_type == 'cosine':
#             # ä½™å¼¦è·ç¦» = 1 - ä½™å¼¦ç›¸ä¼¼åº¦
#             cos_sim = F.cosine_similarity(x1, x2, dim=1)
#             distance = 1 - cos_sim
#         else:
#             raise ValueError(f'Unknown distance type: {self.distance_type}')
#
#         return distance
#
#     def forward(self, x1, x2, labels):
#         """
#         è®¡ç®—å¯¹æ¯”æŸå¤±
#
#         Args:
#             x1: [batch, embedding_dim]ï¼Œç¬¬ä¸€ä¸ªåµŒå…¥
#             x2: [batch, embedding_dim]ï¼Œç¬¬äºŒä¸ªåµŒå…¥
#             labels: [batch]ï¼Œ0è¡¨ç¤ºåŒä¸€è¯´è¯äººï¼Œ1è¡¨ç¤ºä¸åŒè¯´è¯äºº
#
#         Returns:
#             loss: scalar
#             stats: dictï¼Œç»Ÿè®¡ä¿¡æ¯
#         """
#         # è®¡ç®—è·ç¦»
#         distance = self.compute_distance(x1, x2)
#
#         # åˆ†ç¦»æ­£è´Ÿæ ·æœ¬
#         positive_mask = (labels == 0).float()  # åŒä¸€è¯´è¯äºº
#         negative_mask = (labels == 1).float()  # ä¸åŒè¯´è¯äºº
#
#         # æ­£æ ·æœ¬æŸå¤±ï¼šè·ç¦»è¶Šå°è¶Šå¥½
#         positive_loss = positive_mask * distance ** 2
#
#         # è´Ÿæ ·æœ¬æŸå¤±ï¼šè·ç¦»å¤§äºmarginæ‰å¥½
#         negative_loss = negative_mask * torch.clamp(self.margin - distance, min=0) ** 2
#
#         # æ€»æŸå¤±
#         total_loss = (positive_loss + negative_loss).mean()
#
#         # ç»Ÿè®¡ä¿¡æ¯
#         with torch.no_grad():
#             num_positives = positive_mask.sum().item()
#             num_negatives = negative_mask.sum().item()
#
#             if num_positives > 0:
#                 avg_pos_dist = (distance * positive_mask).sum().item() / num_positives
#             else:
#                 avg_pos_dist = 0.0
#
#             if num_negatives > 0:
#                 avg_neg_dist = (distance * negative_mask).sum().item() / num_negatives
#             else:
#                 avg_neg_dist = 0.0
#
#         stats = {
#             'loss': total_loss.item(),
#             'avg_positive_distance': avg_pos_dist,
#             'avg_negative_distance': avg_neg_dist,
#             'num_positives': num_positives,
#             'num_negatives': num_negatives
#         }
#
#         return total_loss, stats
#
#
# class TripletContrastiveLoss(nn.Module):
#     """
#     ä¸‰å…ƒç»„å¯¹æ¯”æŸå¤±
#
#     é€‚ç”¨äºtripletæ•°æ®ï¼šanchor, positive, negative
#
#     å…¬å¼:
#     L = max(0, d(anchor, positive) - d(anchor, negative) + margin)
#
#     ç›®æ ‡ï¼šä½¿d(anchor, positive) + margin < d(anchor, negative)
#     """
#
#     def __init__(self, margin=0.5, distance_type='euclidean'):
#         super().__init__()
#         self.margin = margin
#         self.distance_type = distance_type
#
#         print(f'ğŸ“ Triplet Contrastive Loss initialized')
#         print(f'   Margin: {margin}')
#         print(f'   Distance: {distance_type}')
#
#     def compute_distance(self, x1, x2):
#         """è®¡ç®—è·ç¦»"""
#         if self.distance_type == 'euclidean':
#             distance = torch.sqrt(torch.sum((x1 - x2) ** 2, dim=1) + 1e-8)
#         elif self.distance_type == 'cosine':
#             cos_sim = F.cosine_similarity(x1, x2, dim=1)
#             distance = 1 - cos_sim
#         else:
#             raise ValueError(f'Unknown distance type: {self.distance_type}')
#
#         return distance
#
#     def forward(self, anchor, positive, negative):
#         """
#         è®¡ç®—ä¸‰å…ƒç»„æŸå¤±
#
#         Args:
#             anchor: [batch, embedding_dim]
#             positive: [batch, embedding_dim]ï¼ŒåŒä¸€è¯´è¯äºº
#             negative: [batch, embedding_dim]ï¼Œä¸åŒè¯´è¯äºº
#
#         Returns:
#             loss: scalar
#             stats: dict
#         """
#         # è®¡ç®—è·ç¦»
#         pos_distance = self.compute_distance(anchor, positive)
#         neg_distance = self.compute_distance(anchor, negative)
#
#         # Triplet loss
#         loss = torch.clamp(pos_distance - neg_distance + self.margin, min=0)
#         total_loss = loss.mean()
#
#         # ç»Ÿè®¡ä¿¡æ¯
#         with torch.no_grad():
#             avg_pos_dist = pos_distance.mean().item()
#             avg_neg_dist = neg_distance.mean().item()
#             num_hard_triplets = (loss > 0).sum().item()
#
#         stats = {
#             'loss': total_loss.item(),
#             'avg_positive_distance': avg_pos_dist,
#             'avg_negative_distance': avg_neg_dist,
#             'num_hard_triplets': num_hard_triplets,
#             'total_triplets': anchor.shape[0]
#         }
#
#         return total_loss, stats
#
#
# def test_contrastive_loss():
#     """æµ‹è¯•å¯¹æ¯”æŸå¤±"""
#     print('=' * 80)
#     print('ğŸ§ª Testing Contrastive Loss')
#     print('=' * 80)
#
#     batch_size = 16
#     embedding_dim = 192
#
#     # === æµ‹è¯•1: æ ‡å‡†å¯¹æ¯”æŸå¤± ===
#     print('\n[Test 1] Standard Contrastive Loss')
#     contrastive = ContrastiveLoss(margin=1.0, distance_type='euclidean')
#
#     x1 = torch.randn(batch_size, embedding_dim)
#     x2 = torch.randn(batch_size, embedding_dim)
#     labels = torch.randint(0, 2, (batch_size,))  # éšæœº0æˆ–1
#
#     loss, stats = contrastive(x1, x2, labels)
#
#     print(f'Loss: {loss.item():.4f}')
#     print(f'Avg positive distance: {stats["avg_positive_distance"]:.4f}')
#     print(f'Avg negative distance: {stats["avg_negative_distance"]:.4f}')
#     print(f'Num positives: {stats["num_positives"]}')
#     print(f'Num negatives: {stats["num_negatives"]}')
#
#     # === æµ‹è¯•2: ä¸‰å…ƒç»„æŸå¤± ===
#     print('\n[Test 2] Triplet Contrastive Loss')
#     triplet_loss = TripletContrastiveLoss(margin=0.5, distance_type='cosine')
#
#     anchor = F.normalize(torch.randn(batch_size, embedding_dim), p=2, dim=1)
#     positive = F.normalize(torch.randn(batch_size, embedding_dim), p=2, dim=1)
#     negative = F.normalize(torch.randn(batch_size, embedding_dim), p=2, dim=1)
#
#     loss, stats = triplet_loss(anchor, positive, negative)
#
#     print(f'Loss: {loss.item():.4f}')
#     print(f'Avg positive distance: {stats["avg_positive_distance"]:.4f}')
#     print(f'Avg negative distance: {stats["avg_negative_distance"]:.4f}')
#     print(f'Hard triplets: {stats["num_hard_triplets"]}/{stats["total_triplets"]}')
#
#     # === æµ‹è¯•3: æ¢¯åº¦æµ‹è¯• ===
#     print('\n[Test 3] Gradient Flow Test')
#     x1_grad = torch.randn(batch_size, embedding_dim, requires_grad=True)
#     x2_grad = torch.randn(batch_size, embedding_dim, requires_grad=True)
#     labels_grad = torch.randint(0, 2, (batch_size,))
#
#     loss, _ = contrastive(x1_grad, x2_grad, labels_grad)
#     loss.backward()
#
#     print(f'x1 gradient: {x1_grad.grad is not None}')
#     print(f'x1 gradient norm: {x1_grad.grad.norm().item():.6f}')
#     print(f'x2 gradient: {x2_grad.grad is not None}')
#     print(f'x2 gradient norm: {x2_grad.grad.norm().item():.6f}')
#
#     print('\n' + '=' * 80)
#     print('âœ… All contrastive loss tests passed!')
#     print('=' * 80)
#
#
# if __name__ == '__main__':
#     test_contrastive_loss()
"""
Contrastive Loss for Speaker Verification

å®ç°ç›‘ç£å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼Œç”¨äºæ›¿ä»£ Triplet Lossã€‚
é€‚ç”¨äºè¯´è¯äººéªŒè¯ä»»åŠ¡ï¼Œæ”¯æŒå¤šæ­£æ ·æœ¬å’Œå¤šè´Ÿæ ·æœ¬ã€‚

å‚è€ƒæ–‡çŒ®ï¼š
- Supervised Contrastive Learning (Khosla et al., NeurIPS 2020)
- é€‚é…ä¸ºè¯´è¯äººéªŒè¯åœºæ™¯

ä½¿ç”¨æ–¹æ³•ï¼š
    loss_fn = ContrastiveLoss(temperature=0.07)
    embeddings = model(audio)  # [batch, embedding_dim]
    labels = speaker_labels    # [batch]
    loss = loss_fn(embeddings, labels)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class ContrastiveLoss(nn.Module):
    """
    ç›‘ç£å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆSupervised Contrastive Lossï¼‰

    ç”¨äºè¯´è¯äººéªŒè¯ä»»åŠ¡ï¼Œé¼“åŠ±åŒä¸€è¯´è¯äººçš„åµŒå…¥ç›¸ä¼¼ï¼Œ
    ä¸åŒè¯´è¯äººçš„åµŒå…¥åˆ†ç¦»ã€‚

    å…¬å¼ï¼š
        L = -1/|P(i)| * Î£_{pâˆˆP(i)} log[ exp(z_iÂ·z_p/Ï„) / Î£_{aâˆˆA(i)} exp(z_iÂ·z_a/Ï„) ]

    å…¶ä¸­ï¼š
        - P(i): ä¸æ ·æœ¬ i åŒç±»çš„æ ·æœ¬é›†åˆï¼ˆä¸åŒ…æ‹¬ i è‡ªå·±ï¼‰
        - A(i): é™¤äº† i ä¹‹å¤–çš„æ‰€æœ‰æ ·æœ¬
        - Ï„: æ¸©åº¦å‚æ•°
        - z_iÂ·z_p: å½’ä¸€åŒ–åçš„ä½™å¼¦ç›¸ä¼¼åº¦

    Args:
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åˆ†å¸ƒçš„å¹³æ»‘åº¦ï¼ˆé»˜è®¤ 0.07ï¼‰
        contrast_mode: å¯¹æ¯”æ¨¡å¼
            - 'all': ä½¿ç”¨æ‰€æœ‰æ ·æœ¬ä½œä¸ºå¯¹æ¯”
            - 'one': åªä½¿ç”¨ä¸€ä¸ªæ­£æ ·æœ¬ï¼ˆç±»ä¼¼ triplet lossï¼‰
        base_temperature: åŸºç¡€æ¸©åº¦ï¼ˆé»˜è®¤ 0.07ï¼‰

    è¾“å…¥ï¼š
        embeddings: [batch_size, embedding_dim] - åµŒå…¥å‘é‡
        labels: [batch_size] - ç±»åˆ«æ ‡ç­¾ï¼ˆè¯´è¯äºº IDï¼‰
        mask: [batch_size, batch_size] - å¯é€‰ï¼Œæ‰‹åŠ¨æŒ‡å®šæ­£æ ·æœ¬å¯¹

    è¾“å‡ºï¼š
        loss: æ ‡é‡å¼ é‡
    """

    def __init__(self, temperature=0.07, contrast_mode='all', base_temperature=0.07):
        super().__init__()
        self.temperature = temperature
        self.contrast_mode = contrast_mode
        self.base_temperature = base_temperature

    def forward(self, embeddings, labels=None, mask=None):
        """
        è®¡ç®— Contrastive Loss

        Args:
            embeddings: [batch_size, embedding_dim] æˆ– [batch_size, n_views, embedding_dim]
            labels: [batch_size] - ç±»åˆ«æ ‡ç­¾
            mask: [batch_size, batch_size] - å¯é€‰çš„æ­£æ ·æœ¬å¯¹æ©ç 

        Returns:
            loss: æ ‡é‡å¼ é‡
        """
        device = embeddings.device

        # å¤„ç†å¤šè§†å›¾æƒ…å†µï¼ˆä¾‹å¦‚ï¼šnoisy + enhancedï¼‰
        if len(embeddings.shape) == 3:
            # [batch_size, n_views, embedding_dim]
            batch_size, n_views, embedding_dim = embeddings.shape
            embeddings = embeddings.view(batch_size * n_views, embedding_dim)

            if labels is not None:
                labels = labels.contiguous().view(-1, 1)
                labels = labels.repeat(n_views, 1).view(-1)
        else:
            # [batch_size, embedding_dim]
            batch_size = embeddings.shape[0]
            n_views = 1

        # L2 å½’ä¸€åŒ–ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        embeddings = F.normalize(embeddings, p=2, dim=1)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ: [batch_size, batch_size]
        similarity_matrix = torch.matmul(embeddings, embeddings.T)

        # åˆ›å»ºæ­£æ ·æœ¬å¯¹æ©ç 
        if mask is None:
            if labels is None:
                raise ValueError("Either labels or mask must be provided")

            # åŸºäºæ ‡ç­¾åˆ›å»ºæ©ç 
            labels = labels.contiguous().view(-1, 1)
            mask = torch.eq(labels, labels.T).float().to(device)
        else:
            mask = mask.float().to(device)

        # ç§»é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
        logits_mask = torch.scatter(
            torch.ones_like(mask),
            1,
            torch.arange(batch_size * n_views).view(-1, 1).to(device),
            0
        )
        mask = mask * logits_mask

        # è®¡ç®— log_prob
        # åˆ†å­ï¼šexp(sim(z_i, z_p) / Ï„) for all positive pairs
        # åˆ†æ¯ï¼šÎ£ exp(sim(z_i, z_a) / Ï„) for all a â‰  i

        # ç¼©æ”¾ç›¸ä¼¼åº¦
        logits = similarity_matrix / self.temperature

        # æ•°å€¼ç¨³å®šæ€§ï¼šå‡å»æœ€å¤§å€¼
        logits_max, _ = torch.max(logits, dim=1, keepdim=True)
        logits = logits - logits_max.detach()

        # è®¡ç®— exp
        exp_logits = torch.exp(logits) * logits_mask

        # åˆ†æ¯ï¼šæ‰€æœ‰æ ·æœ¬çš„ exp ä¹‹å’Œ
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æ­£æ ·æœ¬å¯¹æ•°é‡
        mask_sum = mask.sum(1)

        # é¿å…é™¤ä»¥0
        mask_sum = torch.where(mask_sum == 0, torch.ones_like(mask_sum), mask_sum)

        # è®¡ç®—å¹³å‡ log-likelihood
        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_sum

        # æŸå¤±ï¼šè´Ÿ log-likelihood
        loss = -(self.temperature / self.base_temperature) * mean_log_prob_pos

        # å¯¹æ‰€æœ‰æ ·æœ¬æ±‚å¹³å‡
        loss = loss.view(n_views, batch_size).mean()

        return loss


class TripletContrastiveLoss(nn.Module):
    """
    ç»“åˆ Triplet Loss å’Œ Contrastive Loss çš„æ··åˆæŸå¤±

    L = Î± * L_triplet + (1-Î±) * L_contrastive

    Args:
        margin: Triplet loss çš„ margin
        temperature: Contrastive loss çš„æ¸©åº¦
        alpha: æƒé‡ç³»æ•° (0-1)ï¼Œalpha=1 æ—¶åªæœ‰ tripletï¼Œalpha=0 æ—¶åªæœ‰ contrastive
    """

    def __init__(self, margin=0.3, temperature=0.07, alpha=0.5):
        super().__init__()
        self.margin = margin
        self.contrastive = ContrastiveLoss(temperature=temperature)
        self.alpha = alpha

    def forward(self, embeddings, labels):
        """
        è®¡ç®—æ··åˆæŸå¤±

        Args:
            embeddings: [batch_size, embedding_dim]
            labels: [batch_size]

        Returns:
            loss: æ ‡é‡
            losses_dict: åŒ…å«å„éƒ¨åˆ†æŸå¤±çš„å­—å…¸
        """
        # Contrastive Loss
        loss_contrastive = self.contrastive(embeddings, labels)

        # Triplet Loss (ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜)
        loss_triplet = self._compute_triplet_loss(embeddings, labels)

        # æ··åˆ
        loss = self.alpha * loss_triplet + (1 - self.alpha) * loss_contrastive

        losses_dict = {
            'total': loss.item(),
            'triplet': loss_triplet.item(),
            'contrastive': loss_contrastive.item()
        }

        return loss, losses_dict

    def _compute_triplet_loss(self, embeddings, labels):
        """è®¡ç®— Triplet Lossï¼ˆç¡¬è´Ÿæ ·æœ¬æŒ–æ˜ï¼‰"""
        # L2 å½’ä¸€åŒ–
        embeddings = F.normalize(embeddings, p=2, dim=1)

        # è®¡ç®—è·ç¦»çŸ©é˜µ
        dist_matrix = torch.cdist(embeddings, embeddings, p=2)

        batch_size = embeddings.shape[0]
        losses = []

        for i in range(batch_size):
            # é”šç‚¹
            anchor_label = labels[i]

            # æ­£æ ·æœ¬ï¼šåŒç±»ä¸”ä¸æ˜¯è‡ªå·±
            pos_mask = (labels == anchor_label) & (torch.arange(batch_size).to(labels.device) != i)
            if pos_mask.sum() == 0:
                continue

            # è´Ÿæ ·æœ¬ï¼šä¸åŒç±»
            neg_mask = labels != anchor_label
            if neg_mask.sum() == 0:
                continue

            # ç¡¬æ­£æ ·æœ¬ï¼ˆæœ€è¿œçš„æ­£æ ·æœ¬ï¼‰
            pos_dists = dist_matrix[i][pos_mask]
            hardest_pos_dist = pos_dists.max()

            # ç¡¬è´Ÿæ ·æœ¬ï¼ˆæœ€è¿‘çš„è´Ÿæ ·æœ¬ï¼‰
            neg_dists = dist_matrix[i][neg_mask]
            hardest_neg_dist = neg_dists.min()

            # Triplet Loss
            loss = torch.clamp(hardest_pos_dist - hardest_neg_dist + self.margin, min=0.0)
            losses.append(loss)

        if len(losses) == 0:
            return torch.tensor(0.0).to(embeddings.device)

        return torch.stack(losses).mean()


def test_contrastive_loss():
    """æµ‹è¯• Contrastive Loss"""
    print('=' * 80)
    print('ğŸ§ª Testing Contrastive Loss')
    print('=' * 80)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # æµ‹è¯•é…ç½®
    batch_size = 32
    embedding_dim = 192
    num_speakers = 8

    print(f'\n[1/4] Creating test data...')
    print(f'   Batch size: {batch_size}')
    print(f'   Embedding dim: {embedding_dim}')
    print(f'   Num speakers: {num_speakers}')

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    embeddings = torch.randn(batch_size, embedding_dim).to(device)
    labels = torch.randint(0, num_speakers, (batch_size,)).to(device)

    print(f'   Embeddings shape: {embeddings.shape}')
    print(f'   Labels shape: {labels.shape}')
    print(f'   Unique speakers: {labels.unique().numel()}')

    # æµ‹è¯• Contrastive Loss
    print(f'\n[2/4] Testing ContrastiveLoss...')
    loss_fn = ContrastiveLoss(temperature=0.07).to(device)

    loss = loss_fn(embeddings, labels)
    print(f'   Loss: {loss.item():.6f}')
    print(f'   âœ… ContrastiveLoss works!')

    # æµ‹è¯•æ¢¯åº¦
    print(f'\n[3/4] Testing gradient flow...')
    embeddings.requires_grad = True
    loss = loss_fn(embeddings, labels)
    loss.backward()

    print(f'   Embeddings has gradient: {embeddings.grad is not None}')
    if embeddings.grad is not None:
        print(f'   Gradient norm: {embeddings.grad.norm().item():.6f}')
        print(f'   âœ… Gradient flows correctly!')

    # æµ‹è¯• TripletContrastiveLoss
    print(f'\n[4/4] Testing TripletContrastiveLoss...')
    embeddings = torch.randn(batch_size, embedding_dim).to(device)
    mixed_loss_fn = TripletContrastiveLoss(margin=0.3, temperature=0.07, alpha=0.5).to(device)

    loss, losses_dict = mixed_loss_fn(embeddings, labels)
    print(f'   Total loss: {losses_dict["total"]:.6f}')
    print(f'   Triplet loss: {losses_dict["triplet"]:.6f}')
    print(f'   Contrastive loss: {losses_dict["contrastive"]:.6f}')
    print(f'   âœ… TripletContrastiveLoss works!')

    print('\n' + '=' * 80)
    print('âœ… All tests passed!')
    print('=' * 80)

    print('\nğŸ’¡ Usage examples:')
    print('\n1. Pure Contrastive Loss:')
    print('   loss_fn = ContrastiveLoss(temperature=0.07)')
    print('   loss = loss_fn(embeddings, labels)')

    print('\n2. Mixed Loss:')
    print('   loss_fn = TripletContrastiveLoss(margin=0.3, temperature=0.07, alpha=0.5)')
    print('   loss, losses_dict = loss_fn(embeddings, labels)')

    print('\n3. Multi-view (noisy + enhanced):')
    print('   embeddings = torch.stack([emb_noisy, emb_enhanced], dim=1)  # [batch, 2, dim]')
    print('   loss = loss_fn(embeddings, labels)')


if __name__ == '__main__':
    test_contrastive_loss()