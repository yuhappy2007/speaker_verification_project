# -*- coding: utf-8 -*-
"""
é˜¶æ®µ1ï¼šå®ç°SupConæŸå¤±æ›¿æ¢Triplet Loss
åŸºäºåŸæœ‰çš„models_fixed.pyè¿›è¡Œæ”¹è¿›
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class SupConLoss(nn.Module):
    """
    Supervised Contrastive Loss (SupCon)
    å‚è€ƒè®ºæ–‡: "Supervised Contrastive Learning" (NeurIPS 2020)

    ä¼˜åŠ¿ï¼š
    1. åŒæ—¶åˆ©ç”¨batchå†…æ‰€æœ‰æ­£è´Ÿæ ·æœ¬å¯¹
    2. è®­ç»ƒæ›´ç¨³å®šï¼Œä¸éœ€è¦å¤æ‚çš„éš¾æ ·æœ¬æŒ–æ˜
    3. å¯¹å™ªå£°æ›´é²æ£’
    """

    def __init__(self, temperature=0.07, base_temperature=0.07):
        """
        Args:
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶åˆ†å¸ƒçš„å°–é”ç¨‹åº¦
                        - è¾ƒå°çš„å€¼(0.05-0.1)ä½¿æ¨¡å‹æ›´å…³æ³¨å›°éš¾æ ·æœ¬
                        - è¾ƒå¤§çš„å€¼(0.5-1.0)ä½¿è®­ç»ƒæ›´å¹³æ»‘
            base_temperature: åŸºç¡€æ¸©åº¦ï¼Œç”¨äºå½’ä¸€åŒ–
        """
        super().__init__()
        self.temperature = temperature
        self.base_temperature = base_temperature

    def forward(self, features, labels):
        """
        è®¡ç®—Supervised Contrastive Loss

        Args:
            features: [batch_size, embedding_dim] - L2å½’ä¸€åŒ–çš„åµŒå…¥å‘é‡
            labels: [batch_size] - è¯´è¯äººæ ‡ç­¾

        Returns:
            loss: æ ‡é‡å¼ é‡
        """
        device = features.device
        batch_size = features.shape[0]

        # ç¡®ä¿featureså·²ç»L2å½’ä¸€åŒ–
        features = F.normalize(features, p=2, dim=1)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ: [batch_size, batch_size]
        similarity_matrix = torch.matmul(features, features.T)

        # åˆ›å»ºmaskï¼šæ ‡è®°å“ªäº›æ ·æœ¬å¯¹æ˜¯åŒä¸€è¯´è¯äººï¼ˆæ­£æ ·æœ¬å¯¹ï¼‰
        # labels: [batch_size] -> [batch_size, 1] -> [batch_size, batch_size]
        labels = labels.contiguous().view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(device)

        # å»é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±å’Œè‡ªå·±ä¸ç®—æ­£æ ·æœ¬å¯¹ï¼‰
        logits_mask = torch.scatter(
            torch.ones_like(mask),
            1,
            torch.arange(batch_size).view(-1, 1).to(device),
            0
        )
        mask = mask * logits_mask

        # è®¡ç®—log_prob
        # é™¤ä»¥æ¸©åº¦å‚æ•°
        similarity_matrix = similarity_matrix / self.temperature

        # ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œå‡å»æœ€å¤§å€¼
        logits_max, _ = torch.max(similarity_matrix, dim=1, keepdim=True)
        logits = similarity_matrix - logits_max.detach()

        # è®¡ç®—exp(similarity)
        exp_logits = torch.exp(logits) * logits_mask

        # è®¡ç®—log(sum(exp(similarity)))ç”¨äºåˆ†æ¯
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)

        # è®¡ç®—æ¯ä¸ªanchorçš„æ­£æ ·æœ¬å¯¹çš„å¹³å‡log_prob
        # mask.sum(1)æ˜¯æ¯ä¸ªanchorçš„æ­£æ ·æœ¬æ•°é‡
        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-12)

        # æŸå¤±æ˜¯è´Ÿçš„å¹³å‡log probability
        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos
        loss = loss.mean()

        return loss


class RobustEmbeddingMLP(nn.Module):
    """
    ä¿æŒä¸åŸè®ºæ–‡ä¸€è‡´çš„MLPç»“æ„
    3å±‚MLPç”¨äºèåˆnoisyå’Œenhanced embeddings
    """

    def __init__(self, embedding_dim=192):
        super().__init__()

        self.fc1 = nn.Linear(embedding_dim * 2, embedding_dim)
        self.fc2 = nn.Linear(embedding_dim, embedding_dim)
        self.fc3 = nn.Linear(embedding_dim, embedding_dim)

        self.relu = nn.ReLU()

        # Xavieråˆå§‹åŒ–
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)
        nn.init.xavier_uniform_(self.fc3.weight)
        nn.init.zeros_(self.fc1.bias)
        nn.init.zeros_(self.fc2.bias)
        nn.init.zeros_(self.fc3.bias)

    def forward(self, noisy_emb, enhanced_emb):
        """
        Args:
            noisy_emb: [batch_size, embedding_dim]
            enhanced_emb: [batch_size, embedding_dim]

        Returns:
            robust_emb: [batch_size, embedding_dim] (L2 normalized)
        """
        x = torch.cat([noisy_emb, enhanced_emb], dim=1)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)

        # L2å½’ä¸€åŒ–ï¼ˆå¯¹SupConå¾ˆé‡è¦ï¼ï¼‰
        x = F.normalize(x, p=2, dim=1)

        return x


class SupConTrainer:
    """
    ä½¿ç”¨SupConæŸå¤±çš„è®­ç»ƒå™¨
    """

    def __init__(self, embedding_dim=192, device='cuda', temperature=0.07):
        self.device = device
        self.embedding_dim = embedding_dim

        # åˆ›å»ºMLP
        self.mlp = RobustEmbeddingMLP(embedding_dim).to(device)

        # ä½¿ç”¨SupConæŸå¤±
        self.criterion = SupConLoss(temperature=temperature)

        self.optimizer = None

        print(f'âœ… SupCon Trainer initialized on {device}')
        print(f'ğŸ“Š MLP parameters: {sum(p.numel() for p in self.mlp.parameters()):,}')
        print(f'ğŸŒ¡ï¸  Temperature: {temperature}')

    def train_step(self, noisy_embs, enhanced_embs, labels):
        """
        æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤

        Args:
            noisy_embs: [batch_size, embedding_dim] - å™ªå£°åµŒå…¥
            enhanced_embs: [batch_size, embedding_dim] - å¢å¼ºåµŒå…¥
            labels: [batch_size] - è¯´è¯äººæ ‡ç­¾

        Returns:
            loss: float - æŸå¤±å€¼
        """
        self.optimizer.zero_grad()

        # é€šè¿‡MLPèåˆå¾—åˆ°é²æ£’åµŒå…¥
        robust_embs = self.mlp(noisy_embs, enhanced_embs)

        # è®¡ç®—SupConæŸå¤±
        loss = self.criterion(robust_embs, labels)

        loss.backward()
        self.optimizer.step()

        return loss.item()

    def extract_embedding(self, noisy_emb, enhanced_emb):
        """
        æ¨ç†æ—¶æå–é²æ£’åµŒå…¥

        Args:
            noisy_emb: [1, embedding_dim] or [embedding_dim]
            enhanced_emb: [1, embedding_dim] or [embedding_dim]

        Returns:
            robust_emb: [1, embedding_dim] æˆ– [embedding_dim]
        """
        self.mlp.eval()
        with torch.no_grad():
            # ç¡®ä¿æ˜¯2Då¼ é‡
            if noisy_emb.dim() == 1:
                noisy_emb = noisy_emb.unsqueeze(0)
            if enhanced_emb.dim() == 1:
                enhanced_emb = enhanced_emb.unsqueeze(0)

            robust_emb = self.mlp(noisy_emb, enhanced_emb)

        return robust_emb


# ============ å¯¹æ¯”ï¼šTriplet Lossç‰ˆæœ¬ï¼ˆç”¨äºbaselineå¯¹æ¯”ï¼‰============

class TripletLossFixed(nn.Module):
    """åŸè®ºæ–‡çš„Triplet Lossï¼ˆä¿®å¤ç‰ˆï¼‰"""

    def __init__(self, margin=0.25, min_neg_dist=0.4):
        super().__init__()
        self.margin = margin
        self.min_neg_dist = min_neg_dist

    def cosine_distance(self, x, y):
        cos_sim = F.cosine_similarity(x, y, dim=1)
        return 1 - cos_sim

    def forward(self, anchor, positive, negative):
        pos_dist = self.cosine_distance(anchor, positive)
        neg_dist = self.cosine_distance(anchor, negative)

        triplet_loss = torch.clamp(pos_dist - neg_dist + self.margin, min=0.0)
        neg_constraint = torch.clamp(self.min_neg_dist - neg_dist, min=0.0)

        total_loss = triplet_loss + 0.5 * neg_constraint
        return total_loss.mean()


class TripletTrainer:
    """åŸè®ºæ–‡çš„Triplet Lossè®­ç»ƒå™¨"""

    def __init__(self, embedding_dim=192, device='cuda',
                 triplet_margin=0.25, min_neg_dist=0.4):
        self.device = device
        self.embedding_dim = embedding_dim

        self.mlp = RobustEmbeddingMLP(embedding_dim).to(device)
        self.criterion = TripletLossFixed(
            margin=triplet_margin,
            min_neg_dist=min_neg_dist
        )
        self.optimizer = None

        print(f'âœ… Triplet Trainer initialized on {device}')
        print(f'ğŸ“Š MLP parameters: {sum(p.numel() for p in self.mlp.parameters()):,}')

    def train_step(self, anchor_robust, positive_robust, negative_robust):
        self.optimizer.zero_grad()
        loss = self.criterion(anchor_robust, positive_robust, negative_robust)
        loss.backward()
        self.optimizer.step()
        return loss.item()


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹ä»£ç """

    # å‡è®¾å‚æ•°
    batch_size = 32
    embedding_dim = 192
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # 1. åˆ›å»ºSupConè®­ç»ƒå™¨
    supcon_trainer = SupConTrainer(
        embedding_dim=embedding_dim,
        device=device,
        temperature=0.07  # å¯è°ƒæ•´ï¼š0.05-0.1
    )

    # 2. è®¾ç½®ä¼˜åŒ–å™¨
    supcon_trainer.optimizer = torch.optim.AdamW(
        supcon_trainer.mlp.parameters(),
        lr=1e-3,
        weight_decay=1e-4
    )

    # 3. æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    noisy_embs = torch.randn(batch_size, embedding_dim).to(device)
    enhanced_embs = torch.randn(batch_size, embedding_dim).to(device)
    labels = torch.randint(0, 10, (batch_size,)).to(device)  # 10ä¸ªè¯´è¯äºº

    # 4. è®­ç»ƒä¸€æ­¥
    loss = supcon_trainer.train_step(noisy_embs, enhanced_embs, labels)
    print(f'SupCon Loss: {loss:.4f}')

    # 5. æ¨ç†
    test_noisy = torch.randn(1, embedding_dim).to(device)
    test_enhanced = torch.randn(1, embedding_dim).to(device)
    robust_emb = supcon_trainer.extract_embedding(test_noisy, test_enhanced)
    print(f'Robust embedding shape: {robust_emb.shape}')


if __name__ == '__main__':
    example_usage()