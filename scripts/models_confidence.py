# """
# é˜¶æ®µ2ä¼˜åŒ–: ç½®ä¿¡åº¦è¯„ä¼°ç½‘ç»œ (Confidence Estimation Network)
#
# æ ¸å¿ƒæ€æƒ³:
# - åŠ¨æ€å­¦ä¹ noisyå’Œenhanced embeddingçš„å¯ä¿¡åº¦
# - é€šè¿‡softmaxç¡®ä¿æƒé‡å’Œä¸º1
# - è½»é‡çº§è®¾è®¡,å¯è§£é‡Šæ€§å¼º
#
# å‚è€ƒæ–‡çŒ®:
# - SENet: Squeeze-and-Excitation Networks (CVPR 2018)
# - Attention mechanisms for multi-modal fusion
# - Dynamic weighting in speaker recognition
#
# è®¾è®¡:
# è¾“å…¥: [E_n; E_e] (concatenated)
# ç½‘ç»œ: 2å±‚MLP + Softmax
# è¾“å‡º: [w_n, w_e] (æƒé‡å’Œä¸º1)
# èåˆ: E_robust = w_n * E_n + w_e * E_e
# """
#
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
#
#
# class ConfidenceEstimationNet(nn.Module):
#     """
#     ç½®ä¿¡åº¦è¯„ä¼°ç½‘ç»œ
#
#     åŠŸèƒ½:
#     1. è¯„ä¼°noisyå’Œenhanced embeddingçš„å¯ä¿¡åº¦
#     2. è¾“å‡ºä¸¤ä¸ªæƒé‡(å’Œä¸º1)ç”¨äºåŠ¨æ€èåˆ
#     3. è½»é‡çº§è®¾è®¡,æ˜“äºè®­ç»ƒ
#
#     æ¶æ„:
#     Input: [batch, 384] (concatenated E_n and E_e)
#     â”œâ”€â”€ FC1: 384 -> 192 + ReLU
#     â”œâ”€â”€ FC2: 192 -> 64 + ReLU
#     â””â”€â”€ FC3: 64 -> 2 + Softmax
#     Output: [batch, 2] (weights for [E_n, E_e])
#     """
#
#     def __init__(self, embedding_dim=192, hidden_dim=64):
#         """
#         Args:
#             embedding_dim: å•ä¸ªembeddingçš„ç»´åº¦(é»˜è®¤192)
#             hidden_dim: éšè—å±‚ç»´åº¦(é»˜è®¤64,å¯è°ƒæ•´)
#         """
#         super().__init__()
#
#         self.embedding_dim = embedding_dim
#         self.hidden_dim = hidden_dim
#
#         # ä¸‰å±‚MLP: é€æ­¥å‹ç¼©ç»´åº¦
#         self.fc1 = nn.Linear(embedding_dim * 2, embedding_dim)
#         self.fc2 = nn.Linear(embedding_dim, hidden_dim)
#         self.fc3 = nn.Linear(hidden_dim, 2)  # è¾“å‡º2ä¸ªæƒé‡
#
#         # BatchNormæå‡ç¨³å®šæ€§(å¯é€‰)
#         self.bn1 = nn.BatchNorm1d(embedding_dim)
#         self.bn2 = nn.BatchNorm1d(hidden_dim)
#
#         # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ(å¯é€‰)
#         self.dropout = nn.Dropout(0.1)
#
#         # Xavieråˆå§‹åŒ–
#         self._init_weights()
#
#     def _init_weights(self):
#         """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
#         for m in self.modules():
#             if isinstance(m, nn.Linear):
#                 nn.init.xavier_uniform_(m.weight)
#                 if m.bias is not None:
#                     nn.init.zeros_(m.bias)
#
#     def forward(self, noisy_emb, enhanced_emb):
#         """
#         å‰å‘ä¼ æ’­
#
#         Args:
#             noisy_emb: [batch, embedding_dim]
#             enhanced_emb: [batch, embedding_dim]
#
#         Returns:
#             weights: [batch, 2] - æƒé‡[w_n, w_e],å’Œä¸º1
#             stats: dict - ç”¨äºåˆ†æçš„ç»Ÿè®¡ä¿¡æ¯
#         """
#         # æ‹¼æ¥ä¸¤ä¸ªembedding
#         x = torch.cat([noisy_emb, enhanced_emb], dim=1)  # [batch, 384]
#
#         # ç¬¬ä¸€å±‚
#         x = self.fc1(x)
#         x = self.bn1(x)
#         x = F.relu(x)
#         x = self.dropout(x)
#
#         # ç¬¬äºŒå±‚
#         x = self.fc2(x)
#         x = self.bn2(x)
#         x = F.relu(x)
#         x = self.dropout(x)
#
#         # è¾“å‡ºå±‚
#         logits = self.fc3(x)  # [batch, 2]
#
#         # Softmaxç¡®ä¿æƒé‡å’Œä¸º1
#         weights = F.softmax(logits, dim=1)  # [batch, 2]
#
#         # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯(ç”¨äºåˆ†æ)
#         stats = {
#             'w_noisy_mean': weights[:, 0].mean().item(),
#             'w_enhanced_mean': weights[:, 1].mean().item(),
#             'w_noisy_std': weights[:, 0].std().item(),
#             'w_enhanced_std': weights[:, 1].std().item(),
#             'logits_mean': logits.mean().item(),
#             'logits_std': logits.std().item(),
#         }
#
#         return weights, stats
#
#
# class DynamicFusionMLP(nn.Module):
#     """
#     åŠ¨æ€èåˆMLP - ç»“åˆç½®ä¿¡åº¦ç½‘ç»œ
#
#     æµç¨‹:
#     1. è¾“å…¥noisyå’Œenhanced embedding
#     2. ç½®ä¿¡åº¦ç½‘ç»œè¯„ä¼°æƒé‡[w_n, w_e]
#     3. åŠ¨æ€èåˆ: E_fused = w_n * E_n + w_e * E_e
#     4. é€šè¿‡ä¸€ä¸ªè½»é‡MLPå¾—åˆ°æœ€ç»ˆrobust embedding
#     5. L2å½’ä¸€åŒ–
#     """
#
#     def __init__(self, embedding_dim=192, confidence_hidden_dim=64):
#         super().__init__()
#
#         self.embedding_dim = embedding_dim
#
#         # ç½®ä¿¡åº¦è¯„ä¼°ç½‘ç»œ
#         self.confidence_net = ConfidenceEstimationNet(
#             embedding_dim=embedding_dim,
#             hidden_dim=confidence_hidden_dim
#         )
#
#         # èåˆåçš„refinement network(å¯é€‰,å¢å¼ºè¡¨è¾¾èƒ½åŠ›)
#         self.refine_fc1 = nn.Linear(embedding_dim, embedding_dim)
#         self.refine_fc2 = nn.Linear(embedding_dim, embedding_dim)
#
#         # Xavieråˆå§‹åŒ–
#         nn.init.xavier_uniform_(self.refine_fc1.weight)
#         nn.init.xavier_uniform_(self.refine_fc2.weight)
#         nn.init.zeros_(self.refine_fc1.bias)
#         nn.init.zeros_(self.refine_fc2.bias)
#
#     def forward(self, noisy_emb, enhanced_emb, return_weights=False):
#         """
#         åŠ¨æ€èåˆå‰å‘ä¼ æ’­
#
#         Args:
#             noisy_emb: [batch, 192]
#             enhanced_emb: [batch, 192]
#             return_weights: æ˜¯å¦è¿”å›ç½®ä¿¡åº¦æƒé‡(ç”¨äºåˆ†æ)
#
#         Returns:
#             robust_emb: [batch, 192] - L2å½’ä¸€åŒ–çš„robust embedding
#             weights (å¯é€‰): [batch, 2] - ç½®ä¿¡åº¦æƒé‡
#             stats (å¯é€‰): dict - ç»Ÿè®¡ä¿¡æ¯
#         """
#         # 1. è¯„ä¼°ç½®ä¿¡åº¦æƒé‡
#         weights, stats = self.confidence_net(noisy_emb, enhanced_emb)
#         w_noisy = weights[:, 0].unsqueeze(1)  # [batch, 1]
#         w_enhanced = weights[:, 1].unsqueeze(1)  # [batch, 1]
#
#         # 2. åŠ¨æ€åŠ æƒèåˆ
#         fused_emb = w_noisy * noisy_emb + w_enhanced * enhanced_emb  # [batch, 192]
#
#         # 3. Refinement (å¯é€‰,å¢å¼ºéçº¿æ€§èƒ½åŠ›)
#         x = F.relu(self.refine_fc1(fused_emb))
#         x = self.refine_fc2(x)
#
#         # 4. æ®‹å·®è¿æ¥(ä¿ç•™åŸå§‹èåˆä¿¡æ¯)
#         robust_emb = fused_emb + x
#
#         # 5. L2å½’ä¸€åŒ–(SupConè¦æ±‚)
#         robust_emb = F.normalize(robust_emb, p=2, dim=1)
#
#         if return_weights:
#             return robust_emb, weights, stats
#         else:
#             return robust_emb
#
#
# class ConfidenceSupConTrainer:
#     """
#     å¸¦ç½®ä¿¡åº¦ç½‘ç»œçš„SupConè®­ç»ƒå™¨
#
#     ç›¸æ¯”åŸå§‹SupConTrainerçš„æ”¹è¿›:
#     1. ä½¿ç”¨DynamicFusionMLPæ›¿ä»£ç®€å•MLP
#     2. å¯ä»¥è®°å½•å’Œåˆ†æç½®ä¿¡åº¦æƒé‡
#     3. æ”¯æŒæƒé‡å¯è§†åŒ–
#     """
#
#     def __init__(self, embedding_dim=192, device='cuda',
#                  temperature=0.07, confidence_hidden_dim=64):
#         self.device = device
#         self.embedding_dim = embedding_dim
#
#         # ä½¿ç”¨åŠ¨æ€èåˆMLP
#         self.mlp = DynamicFusionMLP(
#             embedding_dim=embedding_dim,
#             confidence_hidden_dim=confidence_hidden_dim
#         ).to(device)
#
#         # SupCon Loss(ä»åŸå§‹å®ç°å¯¼å…¥)
#         from models_supcon import SupConLoss
#         self.criterion = SupConLoss(temperature=temperature)
#
#         self.optimizer = None
#
#         # ç”¨äºè®°å½•æƒé‡ç»Ÿè®¡
#         self.weight_history = []
#
#         print(f'âœ… Confidence SupCon Trainer initialized on {device}')
#         print(f'ğŸ“Š Model architecture:')
#         print(f'   - Confidence Network: {sum(p.numel() for p in self.mlp.confidence_net.parameters()):,} params')
#         print(
#             f'   - Refinement Network: {sum(p.numel() for p in [self.mlp.refine_fc1.parameters(), self.mlp.refine_fc2.parameters()]):,} params')
#         print(f'   - Total MLP params: {sum(p.numel() for p in self.mlp.parameters()):,}')
#         print(f'ğŸŒ¡ï¸  Temperature: {temperature}')
#
#     def train_step(self, noisy_embs, enhanced_embs, labels, log_weights=False):
#         """
#         æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤
#
#         Args:
#             noisy_embs: [batch_size, embedding_dim]
#             enhanced_embs: [batch_size, embedding_dim]
#             labels: [batch_size]
#             log_weights: æ˜¯å¦è®°å½•æƒé‡ç»Ÿè®¡
#
#         Returns:
#             loss: float - æŸå¤±å€¼
#             stats: dict - ç»Ÿè®¡ä¿¡æ¯(åŒ…å«æƒé‡ä¿¡æ¯)
#         """
#         self.optimizer.zero_grad()
#
#         # é€šè¿‡åŠ¨æ€èåˆMLPè·å¾—robust embeddings
#         if log_weights:
#             robust_embs, weights, weight_stats = self.mlp(
#                 noisy_embs, enhanced_embs, return_weights=True
#             )
#         else:
#             robust_embs = self.mlp(noisy_embs, enhanced_embs, return_weights=False)
#             weight_stats = {}
#
#         # è®¡ç®—SupConæŸå¤±
#         loss = self.criterion(robust_embs, labels)
#
#         loss.backward()
#         self.optimizer.step()
#
#         # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
#         stats = {
#             'loss': loss.item(),
#             **weight_stats
#         }
#
#         if log_weights:
#             self.weight_history.append(weight_stats)
#
#         return loss.item(), stats
#
#     def extract_embedding(self, noisy_emb, enhanced_emb):
#         """
#         æ¨ç†æ—¶æå–robust embedding
#
#         Args:
#             noisy_emb: [1, embedding_dim] or [embedding_dim]
#             enhanced_emb: [1, embedding_dim] or [embedding_dim]
#
#         Returns:
#             robust_emb: [1, embedding_dim] æˆ– [embedding_dim]
#             weights: [1, 2] - ç½®ä¿¡åº¦æƒé‡
#         """
#         self.mlp.eval()
#         with torch.no_grad():
#             # ç¡®ä¿æ˜¯2Då¼ é‡
#             if noisy_emb.dim() == 1:
#                 noisy_emb = noisy_emb.unsqueeze(0)
#             if enhanced_emb.dim() == 1:
#                 enhanced_emb = enhanced_emb.unsqueeze(0)
#
#             robust_emb, weights, _ = self.mlp(
#                 noisy_emb, enhanced_emb, return_weights=True
#             )
#
#         return robust_emb, weights
#
#     def get_weight_statistics(self):
#         """è·å–è®­ç»ƒè¿‡ç¨‹ä¸­çš„æƒé‡ç»Ÿè®¡"""
#         if not self.weight_history:
#             return None
#
#         import numpy as np
#
#         w_noisy_vals = [h['w_noisy_mean'] for h in self.weight_history]
#         w_enhanced_vals = [h['w_enhanced_mean'] for h in self.weight_history]
#
#         return {
#             'w_noisy': {
#                 'mean': np.mean(w_noisy_vals),
#                 'std': np.std(w_noisy_vals),
#                 'min': np.min(w_noisy_vals),
#                 'max': np.max(w_noisy_vals),
#             },
#             'w_enhanced': {
#                 'mean': np.mean(w_enhanced_vals),
#                 'std': np.std(w_enhanced_vals),
#                 'min': np.min(w_enhanced_vals),
#                 'max': np.max(w_enhanced_vals),
#             }
#         }
#
#
# # ============ ä½¿ç”¨ç¤ºä¾‹ ============
#
# def example_usage():
#     """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ç½®ä¿¡åº¦ç½‘ç»œ"""
#
#     batch_size = 32
#     embedding_dim = 192
#     device = 'cuda' if torch.cuda.is_available() else 'cpu'
#
#     # 1. åˆ›å»ºè®­ç»ƒå™¨
#     trainer = ConfidenceSupConTrainer(
#         embedding_dim=embedding_dim,
#         device=device,
#         temperature=0.07,
#         confidence_hidden_dim=64
#     )
#
#     # 2. è®¾ç½®ä¼˜åŒ–å™¨
#     trainer.optimizer = torch.optim.AdamW(
#         trainer.mlp.parameters(),
#         lr=1e-3,
#         weight_decay=1e-4
#     )
#
#     # 3. æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
#     noisy_embs = torch.randn(batch_size, embedding_dim).to(device)
#     enhanced_embs = torch.randn(batch_size, embedding_dim).to(device)
#     labels = torch.randint(0, 10, (batch_size,)).to(device)
#
#     # 4. è®­ç»ƒä¸€æ­¥
#     loss, stats = trainer.train_step(
#         noisy_embs, enhanced_embs, labels, log_weights=True
#     )
#
#     print(f'\nğŸ“Š Training step results:')
#     print(f'   Loss: {loss:.4f}')
#     print(f'   Avg weight (noisy): {stats["w_noisy_mean"]:.4f}')
#     print(f'   Avg weight (enhanced): {stats["w_enhanced_mean"]:.4f}')
#
#     # 5. æ¨ç†
#     test_noisy = torch.randn(1, embedding_dim).to(device)
#     test_enhanced = torch.randn(1, embedding_dim).to(device)
#     robust_emb, weights = trainer.extract_embedding(test_noisy, test_enhanced)
#
#     print(f'\nğŸ” Inference results:')
#     print(f'   Robust embedding shape: {robust_emb.shape}')
#     print(f'   Confidence weights: noisy={weights[0, 0]:.4f}, enhanced={weights[0, 1]:.4f}')
#
#
# if __name__ == '__main__':
#     example_usage()
"""
é˜¶æ®µ2ä¼˜åŒ–: ç½®ä¿¡åº¦è¯„ä¼°ç½‘ç»œ (Confidence Estimation Network)

æ ¸å¿ƒæ€æƒ³:
- åŠ¨æ€å­¦ä¹ noisyå’Œenhanced embeddingçš„å¯ä¿¡åº¦
- é€šè¿‡softmaxç¡®ä¿æƒé‡å’Œä¸º1
- è½»é‡çº§è®¾è®¡,å¯è§£é‡Šæ€§å¼º

å‚è€ƒæ–‡çŒ®:
- SENet: Squeeze-and-Excitation Networks (CVPR 2018)
- Attention mechanisms for multi-modal fusion
- Dynamic weighting in speaker recognition

è®¾è®¡:
è¾“å…¥: [E_n; E_e] (concatenated)
ç½‘ç»œ: 2å±‚MLP + Softmax
è¾“å‡º: [w_n, w_e] (æƒé‡å’Œä¸º1)
èåˆ: E_robust = w_n * E_n + w_e * E_e
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class ConfidenceEstimationNet(nn.Module):
    """
    ç½®ä¿¡åº¦è¯„ä¼°ç½‘ç»œ

    åŠŸèƒ½:
    1. è¯„ä¼°noisyå’Œenhanced embeddingçš„å¯ä¿¡åº¦
    2. è¾“å‡ºä¸¤ä¸ªæƒé‡(å’Œä¸º1)ç”¨äºåŠ¨æ€èåˆ
    3. è½»é‡çº§è®¾è®¡,æ˜“äºè®­ç»ƒ

    æ¶æ„:
    Input: [batch, 384] (concatenated E_n and E_e)
    â”œâ”€â”€ FC1: 384 -> 192 + ReLU
    â”œâ”€â”€ FC2: 192 -> 64 + ReLU
    â””â”€â”€ FC3: 64 -> 2 + Softmax
    Output: [batch, 2] (weights for [E_n, E_e])
    """

    def __init__(self, embedding_dim=192, hidden_dim=64):
        """
        Args:
            embedding_dim: å•ä¸ªembeddingçš„ç»´åº¦(é»˜è®¤192)
            hidden_dim: éšè—å±‚ç»´åº¦(é»˜è®¤64,å¯è°ƒæ•´)
        """
        super().__init__()

        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        # ä¸‰å±‚MLP: é€æ­¥å‹ç¼©ç»´åº¦
        self.fc1 = nn.Linear(embedding_dim * 2, embedding_dim)
        self.fc2 = nn.Linear(embedding_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 2)  # è¾“å‡º2ä¸ªæƒé‡

        # BatchNormæå‡ç¨³å®šæ€§(å¯é€‰)
        self.bn1 = nn.BatchNorm1d(embedding_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)

        # Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ(å¯é€‰)
        self.dropout = nn.Dropout(0.1)

        # Xavieråˆå§‹åŒ–
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, noisy_emb, enhanced_emb):
        """
        å‰å‘ä¼ æ’­

        Args:
            noisy_emb: [batch, embedding_dim]
            enhanced_emb: [batch, embedding_dim]

        Returns:
            weights: [batch, 2] - æƒé‡[w_n, w_e],å’Œä¸º1
            stats: dict - ç”¨äºåˆ†æçš„ç»Ÿè®¡ä¿¡æ¯
        """
        # æ‹¼æ¥ä¸¤ä¸ªembedding
        x = torch.cat([noisy_emb, enhanced_emb], dim=1)  # [batch, 384]

        # ç¬¬ä¸€å±‚
        x = self.fc1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)

        # ç¬¬äºŒå±‚
        x = self.fc2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.dropout(x)

        # è¾“å‡ºå±‚
        logits = self.fc3(x)  # [batch, 2]

        # Softmaxç¡®ä¿æƒé‡å’Œä¸º1
        weights = F.softmax(logits, dim=1)  # [batch, 2]

        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯(ç”¨äºåˆ†æ)
        stats = {
            'w_noisy_mean': weights[:, 0].mean().item(),
            'w_enhanced_mean': weights[:, 1].mean().item(),
            'w_noisy_std': weights[:, 0].std().item(),
            'w_enhanced_std': weights[:, 1].std().item(),
            'logits_mean': logits.mean().item(),
            'logits_std': logits.std().item(),
        }

        return weights, stats


class DynamicFusionMLP(nn.Module):
    """
    åŠ¨æ€èåˆMLP - ç»“åˆç½®ä¿¡åº¦ç½‘ç»œ

    æµç¨‹:
    1. è¾“å…¥noisyå’Œenhanced embedding
    2. ç½®ä¿¡åº¦ç½‘ç»œè¯„ä¼°æƒé‡[w_n, w_e]
    3. åŠ¨æ€èåˆ: E_fused = w_n * E_n + w_e * E_e
    4. é€šè¿‡ä¸€ä¸ªè½»é‡MLPå¾—åˆ°æœ€ç»ˆrobust embedding
    5. L2å½’ä¸€åŒ–
    """

    def __init__(self, embedding_dim=192, confidence_hidden_dim=64):
        super().__init__()

        self.embedding_dim = embedding_dim

        # ç½®ä¿¡åº¦è¯„ä¼°ç½‘ç»œ
        self.confidence_net = ConfidenceEstimationNet(
            embedding_dim=embedding_dim,
            hidden_dim=confidence_hidden_dim
        )

        # èåˆåçš„refinement network(å¯é€‰,å¢å¼ºè¡¨è¾¾èƒ½åŠ›)
        self.refine_fc1 = nn.Linear(embedding_dim, embedding_dim)
        self.refine_fc2 = nn.Linear(embedding_dim, embedding_dim)

        # Xavieråˆå§‹åŒ–
        nn.init.xavier_uniform_(self.refine_fc1.weight)
        nn.init.xavier_uniform_(self.refine_fc2.weight)
        nn.init.zeros_(self.refine_fc1.bias)
        nn.init.zeros_(self.refine_fc2.bias)

    def forward(self, noisy_emb, enhanced_emb, return_weights=False):
        """
        åŠ¨æ€èåˆå‰å‘ä¼ æ’­

        Args:
            noisy_emb: [batch, 192]
            enhanced_emb: [batch, 192]
            return_weights: æ˜¯å¦è¿”å›ç½®ä¿¡åº¦æƒé‡(ç”¨äºåˆ†æ)

        Returns:
            robust_emb: [batch, 192] - L2å½’ä¸€åŒ–çš„robust embedding
            weights (å¯é€‰): [batch, 2] - ç½®ä¿¡åº¦æƒé‡
            stats (å¯é€‰): dict - ç»Ÿè®¡ä¿¡æ¯
        """
        # 1. è¯„ä¼°ç½®ä¿¡åº¦æƒé‡
        weights, stats = self.confidence_net(noisy_emb, enhanced_emb)
        w_noisy = weights[:, 0].unsqueeze(1)  # [batch, 1]
        w_enhanced = weights[:, 1].unsqueeze(1)  # [batch, 1]

        # 2. åŠ¨æ€åŠ æƒèåˆ
        fused_emb = w_noisy * noisy_emb + w_enhanced * enhanced_emb  # [batch, 192]

        # 3. Refinement (å¯é€‰,å¢å¼ºéçº¿æ€§èƒ½åŠ›)
        x = F.relu(self.refine_fc1(fused_emb))
        x = self.refine_fc2(x)

        # 4. æ®‹å·®è¿æ¥(ä¿ç•™åŸå§‹èåˆä¿¡æ¯)
        robust_emb = fused_emb + x

        # 5. L2å½’ä¸€åŒ–(SupConè¦æ±‚)
        robust_emb = F.normalize(robust_emb, p=2, dim=1)

        if return_weights:
            return robust_emb, weights, stats
        else:
            return robust_emb


class ConfidenceSupConTrainer:
    """
    å¸¦ç½®ä¿¡åº¦ç½‘ç»œçš„SupConè®­ç»ƒå™¨

    ç›¸æ¯”åŸå§‹SupConTrainerçš„æ”¹è¿›:
    1. ä½¿ç”¨DynamicFusionMLPæ›¿ä»£ç®€å•MLP
    2. å¯ä»¥è®°å½•å’Œåˆ†æç½®ä¿¡åº¦æƒé‡
    3. æ”¯æŒæƒé‡å¯è§†åŒ–
    """

    def __init__(self, embedding_dim=192, device='cuda',
                 temperature=0.07, confidence_hidden_dim=64):
        self.device = device
        self.embedding_dim = embedding_dim

        # ä½¿ç”¨åŠ¨æ€èåˆMLP
        self.mlp = DynamicFusionMLP(
            embedding_dim=embedding_dim,
            confidence_hidden_dim=confidence_hidden_dim
        ).to(device)

        # SupCon Loss(ä»åŸå§‹å®ç°å¯¼å…¥)
        from models_supcon import SupConLoss
        self.criterion = SupConLoss(temperature=temperature)

        self.optimizer = None

        # ç”¨äºè®°å½•æƒé‡ç»Ÿè®¡
        self.weight_history = []

        # âœ… ä¿®å¤: æ­£ç¡®è®¡ç®—å‚æ•°é‡
        confidence_params = sum(p.numel() for p in self.mlp.confidence_net.parameters())
        refine_params = sum(p.numel() for p in self.mlp.refine_fc1.parameters()) + \
                        sum(p.numel() for p in self.mlp.refine_fc2.parameters())
        total_params = sum(p.numel() for p in self.mlp.parameters())

        print(f'âœ… Confidence SupCon Trainer initialized on {device}')
        print(f'ğŸ“Š Model architecture:')
        print(f'   - Confidence Network: {confidence_params:,} params')
        print(f'   - Refinement Network: {refine_params:,} params')
        print(f'   - Total MLP params: {total_params:,}')
        print(f'ğŸŒ¡ï¸  Temperature: {temperature}')

    def train_step(self, noisy_embs, enhanced_embs, labels, log_weights=False):
        """
        æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤

        Args:
            noisy_embs: [batch_size, embedding_dim]
            enhanced_embs: [batch_size, embedding_dim]
            labels: [batch_size]
            log_weights: æ˜¯å¦è®°å½•æƒé‡ç»Ÿè®¡

        Returns:
            loss: float - æŸå¤±å€¼
            stats: dict - ç»Ÿè®¡ä¿¡æ¯(åŒ…å«æƒé‡ä¿¡æ¯)
        """
        self.optimizer.zero_grad()

        # é€šè¿‡åŠ¨æ€èåˆMLPè·å¾—robust embeddings
        if log_weights:
            robust_embs, weights, weight_stats = self.mlp(
                noisy_embs, enhanced_embs, return_weights=True
            )
        else:
            robust_embs = self.mlp(noisy_embs, enhanced_embs, return_weights=False)
            weight_stats = {}

        # è®¡ç®—SupConæŸå¤±
        loss = self.criterion(robust_embs, labels)

        loss.backward()
        self.optimizer.step()

        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'loss': loss.item(),
            **weight_stats
        }

        if log_weights:
            self.weight_history.append(weight_stats)

        return loss.item(), stats

    def extract_embedding(self, noisy_emb, enhanced_emb):
        """
        æ¨ç†æ—¶æå–robust embedding

        Args:
            noisy_emb: [1, embedding_dim] or [embedding_dim]
            enhanced_emb: [1, embedding_dim] or [embedding_dim]

        Returns:
            robust_emb: [1, embedding_dim] æˆ– [embedding_dim]
            weights: [1, 2] - ç½®ä¿¡åº¦æƒé‡
        """
        self.mlp.eval()
        with torch.no_grad():
            # ç¡®ä¿æ˜¯2Då¼ é‡
            if noisy_emb.dim() == 1:
                noisy_emb = noisy_emb.unsqueeze(0)
            if enhanced_emb.dim() == 1:
                enhanced_emb = enhanced_emb.unsqueeze(0)

            robust_emb, weights, _ = self.mlp(
                noisy_emb, enhanced_emb, return_weights=True
            )

        return robust_emb, weights

    def get_weight_statistics(self):
        """è·å–è®­ç»ƒè¿‡ç¨‹ä¸­çš„æƒé‡ç»Ÿè®¡"""
        if not self.weight_history:
            return None

        import numpy as np

        w_noisy_vals = [h['w_noisy_mean'] for h in self.weight_history]
        w_enhanced_vals = [h['w_enhanced_mean'] for h in self.weight_history]

        return {
            'w_noisy': {
                'mean': np.mean(w_noisy_vals),
                'std': np.std(w_noisy_vals),
                'min': np.min(w_noisy_vals),
                'max': np.max(w_noisy_vals),
            },
            'w_enhanced': {
                'mean': np.mean(w_enhanced_vals),
                'std': np.std(w_enhanced_vals),
                'min': np.min(w_enhanced_vals),
                'max': np.max(w_enhanced_vals),
            }
        }


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

def example_usage():
    """æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ç½®ä¿¡åº¦ç½‘ç»œ"""

    batch_size = 32
    embedding_dim = 192
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    print('=' * 80)
    print('ğŸ§ª Testing Confidence Estimation Network')
    print('=' * 80)

    # 1. åˆ›å»ºè®­ç»ƒå™¨
    trainer = ConfidenceSupConTrainer(
        embedding_dim=embedding_dim,
        device=device,
        temperature=0.07,
        confidence_hidden_dim=64
    )

    # 2. è®¾ç½®ä¼˜åŒ–å™¨
    trainer.optimizer = torch.optim.AdamW(
        trainer.mlp.parameters(),
        lr=1e-3,
        weight_decay=1e-4
    )

    print('\n' + '=' * 80)
    print('ğŸ“ Training Step Test')
    print('=' * 80)

    # 3. æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
    noisy_embs = torch.randn(batch_size, embedding_dim).to(device)
    enhanced_embs = torch.randn(batch_size, embedding_dim).to(device)
    labels = torch.randint(0, 10, (batch_size,)).to(device)

    # 4. è®­ç»ƒä¸€æ­¥
    loss, stats = trainer.train_step(
        noisy_embs, enhanced_embs, labels, log_weights=True
    )

    print(f'\nğŸ“Š Training results:')
    print(f'   Loss: {loss:.4f}')
    print(f'   Avg weight (noisy): {stats["w_noisy_mean"]:.4f} Â± {stats["w_noisy_std"]:.4f}')
    print(f'   Avg weight (enhanced): {stats["w_enhanced_mean"]:.4f} Â± {stats["w_enhanced_std"]:.4f}')
    print(f'   Weight sum check: {stats["w_noisy_mean"] + stats["w_enhanced_mean"]:.4f} (should be ~1.0)')

    print('\n' + '=' * 80)
    print('ğŸ” Inference Test')
    print('=' * 80)

    # 5. æ¨ç†æµ‹è¯•
    test_noisy = torch.randn(1, embedding_dim).to(device)
    test_enhanced = torch.randn(1, embedding_dim).to(device)
    robust_emb, weights = trainer.extract_embedding(test_noisy, test_enhanced)

    print(f'\nğŸ¯ Inference results:')
    print(f'   Robust embedding shape: {robust_emb.shape}')
    print(f'   Robust embedding norm: {torch.norm(robust_emb, p=2).item():.4f} (should be ~1.0)')
    print(f'   Confidence weights:')
    print(f'      - Noisy: {weights[0, 0]:.4f}')
    print(f'      - Enhanced: {weights[0, 1]:.4f}')
    print(f'      - Sum: {weights[0, 0] + weights[0, 1]:.4f}')

    print('\n' + '=' * 80)
    print('âœ… All tests passed!')
    print('=' * 80)

    print('\nğŸ’¡ Next steps:')
    print('   1. Run: python train_confidence.py (train the model)')
    print('   2. Run: python evaluate_confidence.py (evaluate performance)')
    print('   3. Run: python visualize_weights.py (analyze weight distribution)')


if __name__ == '__main__':
    example_usage()